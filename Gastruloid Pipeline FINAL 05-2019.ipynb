{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import tifffile as tiff\n",
    "from scipy import ndimage\n",
    "from scipy import stats\n",
    "from scipy.integrate import trapz\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import mahotas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the files (16 and 8bit, unblurred, separated channels)\n",
    "##### 16-bit are used to calculate fluorescence, 8-bit for all other measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## location of the 8-bit split channels, per channel\n",
    "\n",
    "data_locn_mask = [\"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1_8bit/Ch0/\", \n",
    "                  \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1_8bit/Ch1/\", \n",
    "                 \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1_8bit/Ch2/\", \n",
    "                  \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1_8bit/Ch3/\"]\n",
    "\n",
    "## location of the 16-bit split channels, per channel\n",
    "data_locn_full = [\"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1/Ch0/\",\n",
    "                 \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1/Ch1/\",\n",
    "                  \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1/Ch2/\",\n",
    "                 \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/3 hGlds Signalling modulation/2019-03-04_Wnt3aPT_05and3Chi_400c_20x_72h_1/Ch3/\"]\n",
    "\n",
    "## location of the 16-bit empty image used for background subtraction, per channel\n",
    "data_locn_bg = [\"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/5 Background Normalisation/Signalling/Ch0/\",\n",
    "                \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/5 Background Normalisation/Signalling/Ch1/\",\n",
    "               \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/5 Background Normalisation/Signalling/Ch2/\",\n",
    "               \"/Users/alexandrabaranowski/Desktop/Cells for pipeline analysis/5 Background Normalisation/Signalling/Ch3/\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retrieving the images from the location and ordering them in ascending order (N.B. Make sure single digit \n",
    "# numbers are labeled 00, 01, 02 etc otherwise they will not be ordered correctly)\n",
    "\n",
    "files_list0 = []; files_list1 = []; files_list2 = []; files_list3 = []\n",
    "files_list_full0 = []; files_list_full1 = []; files_list_full2 = []; files_list_full3 = []\n",
    "bg_list0 = []; bg_list1 = []; bg_list2 = []; bg_list3 = []; \n",
    "\n",
    "files_list = [files_list0, files_list1, files_list2, files_list3]\n",
    "files_list_full = [files_list_full0, files_list_full1, files_list_full2, files_list_full3]\n",
    "bg_list = [bg_list0, bg_list1, bg_list2, bg_list3]\n",
    "\n",
    "for i in range(0, len(channels)):\n",
    "    files_list[i].append(glob.glob(data_locn_mask[i]+'*tif'))\n",
    "    files_list[i][0].sort()\n",
    "    \n",
    "    files_list_full[i].append(glob.glob(data_locn_full[i]+'*tif'))\n",
    "    files_list_full[i][0].sort()\n",
    "        \n",
    "    bg_list[i].append(glob.glob(data_locn_bg[i]+'*tif'))\n",
    "    bg_list[i][0].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames0 = []; filenames1 = []; filenames2 = []; filenames3 = []\n",
    "filenames_full0 = []; filenames_full1 = []; filenames_full2 = []; filenames_full3 = []\n",
    "\n",
    "filenames = [filenames0, filenames1, filenames2, filenames3]\n",
    "filenames_full = [filenames_full0, filenames_full1, filenames_full2, filenames_full3]\n",
    "\n",
    "for j in range(0,len(channels)):\n",
    "    for i in files_list[j][0]:\n",
    "        base = os.path.basename(i)\n",
    "        base2 = os.path.splitext(base)[0]\n",
    "        filenames[j].append(os.path.splitext(base2)[0])\n",
    "\n",
    "for j in range(0,len(channels)):\n",
    "    for i in files_list_full[j][0]:\n",
    "        base = os.path.basename(i)\n",
    "        base2 = os.path.splitext(base)[0]\n",
    "        filenames_full[j].append(os.path.splitext(base2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that the list names are int the correct order - if the files were not named 00, 01 etc. they will not be\n",
    "\n",
    "print files_list[0]\n",
    "files_list_full[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Every channel is processed separately, starting with the empty channel; the pipeline follows the format of: \n",
    "1) Removal of noise <br>\n",
    "2) Thresholding <br>\n",
    "3) Feature Extraction <br>\n",
    "4) QC to ensure all feature lists are of the correct length \n",
    "#### Fluorescence measurements are taken together for all channels \n",
    "5) Fluorescence Noise Removal, Normalisation and Measurements <br>\n",
    "6) Proportion of gastruloid covered by each channel\n",
    "#### Then, general QC is performed to remove images without gastruloids, with debris etc.\n",
    "7) Different methods are used and evaluated <br>\n",
    "8) Based on the one with best results, those indices are removed from all other lists <br>\n",
    "9) Extract proportions and proportion of overlap measurements <br>\n",
    "10) Features are compiled into a dataframe, ready for export to R <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empty Channel - Removing noise & Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) De-noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline \n",
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "blurred_0 = []\n",
    "image_copies_0 = []\n",
    "masks_0 = []\n",
    "blurred_masks_0 = []\n",
    "threshold_0 = []\n",
    "\n",
    "for i, v in enumerate(files_list[0][0]):\n",
    "    # read the original image\n",
    "    image_0 = np.array(Image.open(files_list[0][0][i]))\n",
    "\n",
    "    \n",
    "    #Make a copy to prevent distortion of the original, bc all the transformations change the image\n",
    "    image_copy = image_0.copy()\n",
    "    image_copies_0.append(image_copy)\n",
    "    image_copy2 = image_0.copy()\n",
    "\n",
    "    \n",
    "    #Gaussian Blur \n",
    "\n",
    "    blur = cv2.GaussianBlur(image_copy,(25,25),3) # change this depending on the image \n",
    "    #blur2 = cv2.GaussianBlur(image_copy,(35,35),0) \n",
    "    #diff = blur - blur2\n",
    "    #imgf = image_copy + diff \n",
    "    blurred_0.append(blur)\n",
    "    \n",
    "    #Set initial threshold \n",
    "    ret, th0 = cv2.threshold(blur, 100, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    threshold_0.append(th0)\n",
    "    mask_inv= cv2.bitwise_not(th0)\n",
    "    \n",
    "    \n",
    "    #blurred mask of the gastruloid - for the fluorescence\n",
    "    blurred_masked_img = cv2.bitwise_and(blur,blur, mask = th0)\n",
    "\n",
    "    # bluured mask of the background - to remove bg noise \n",
    "    #blurred_bg_mask = cv2.bitwise_and(blur,blur, mask = mask_inv)      \n",
    "    blurred_masks_0.append(blurred_masked_img)\n",
    "\n",
    "    #fig, ax = plt.subplots(1, 4, figsize = (10,3)) \n",
    "    #ax[0].imshow(image_0)\n",
    "    #ax[1].imshow(th0)\n",
    "    #ax[2].imshow(th1)\n",
    "    #ax[3].imshow(imgf)\n",
    "    #ax[3].imshow(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram-based determination of the correct threshold for segmentation ###\n",
    "### If the image after Gaussian blur looks bimodal, then use Otsu's ###\n",
    "# Otsu's will set threshold automatically; if not, use historgrams to determine threshold to use #\n",
    "\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html\n",
    "for i, v in enumerate(blurred_0[0:6]): # choose a subset of the images or all of them\n",
    "    \n",
    "    ## finding the range of the grayscale values so we can do the thresholding properly \n",
    "    # https://mmeysenburg.github.io/image-processing/05-creating-histograms/ \n",
    "    hist = cv2.calcHist([v], [0], None, [256], [0, 256])\n",
    "    histogram2=cv2.calcHist(image_copies_0[i], [0], None, [256], [0, 256])\n",
    "    fig, ax = plt.subplots(1,2,figsize=(10,3))\n",
    "    \n",
    "    ax[0].plot(histogram2)\n",
    "    ax[0].set_title(\"Grayscale Histogram\") # Before de-noise\n",
    "    ax[0].set_xlabel(\"Grayscale Value\")\n",
    "    ax[0].set_ylabel(\"Pixels\")\n",
    "    ax[1].plot(hist)\n",
    "    ax[1].set_title(\"Grayscale Blurred Histogram\") # After Gaussian Filter\n",
    "    ax[1].set_xlabel(\"Grayscale Value\")\n",
    "    ax[1].set_ylabel(\"Pixels\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "save_locn = \"/Users/alexandrabaranowski/Desktop/Images for report /\"\n",
    "#fig.savefig(save_locn+'1. 2015BraGFP 96h Historgram Gaussian3.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you have identified the threshold, identify the gastruloid as the entity within the frame with the largest area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splprep, splev\n",
    "im_out_ = [] # stores the initial floodfilled mask - this will suffice for some gastruloids\n",
    "contours_0rough = [] # only needed if contour smoothing is applied\n",
    "contours_0 = [] # stores the contours for downstream use\n",
    "erosions = [] # stores the mask after erosions to remove debris surrounding gastruloids \n",
    "smoothed_0 = [] # stores the smoothed contours\n",
    "\n",
    "for i, v in enumerate(image_copies_0):\n",
    "    blurred_copy = v.copy()\n",
    "    blurred_copy1 = v.copy()\n",
    "    image_copy = v.copy()\n",
    "    v2 = v.copy()\n",
    "    \n",
    "    #blur = cv2.GaussianBlur(image_copy, (35,35), 0)\n",
    "    \n",
    "    # Try DoG (imgf) instead of simple Gaussian when Glds have debris around them\n",
    "    blur = cv2.GaussianBlur(image_copy,(25,25),3) \n",
    "    #blur2 = cv2.GaussianBlur(image_copy,(35,35),0) \n",
    "    #diff = blur - blur2\n",
    "    #imgf = image_copy + diff\n",
    "    \n",
    "    \n",
    "    # The threshold can be kept at 255 bc the Otsu's binarisation algorithm finds the optimal threshold for each image\n",
    "    # this is returned as the ret value \n",
    "    ret, th01 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    mask_inv = cv2.bitwise_not(th0)\n",
    "    th0 = cv2.erode(th01, (5,5) ,iterations =1)\n",
    "    \n",
    "    im_floodfill_0 = th0.copy()\n",
    "    \n",
    "    # Mask used for flood filling; Notice the size needs to be 2 pixels larger than the image.\n",
    "    h, w, = th0.shape[:2]\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "\n",
    "    # Floodfill from point (0, 0)\n",
    "    cv2.floodFill(im_floodfill_0, mask, (0,0), 255)\n",
    "\n",
    "    # Invert floodfilled image\n",
    "    im_floodfill_inv_0 = cv2.bitwise_not(im_floodfill_0)\n",
    "\n",
    "    # Combine the two images to get the foreground\n",
    "    im_out_0 = th0 | im_floodfill_inv_0\n",
    "    im_out_.append(im_out_0)\n",
    "    \n",
    "    #### Cleaning the mask - some gastruloids will NOT require this (e.g., if they are clearly distinguishable from\n",
    "    # background, have smooth edges etc)\n",
    "    kernel = np.ones((25,25),np.uint8)\n",
    "    erosion = cv2.erode(im_out_0, kernel ,iterations =1)\n",
    "    #erosion = cv2.morphologyEx(im_out, cv2.MORPH_OPEN, kernel, iterations = 1)\n",
    "    #erosion = cv2.morphologyEx(erosion, cv2.MORPH_OPEN, kernel, iterations = 1)\n",
    "    erosions.append(erosion)\n",
    "    \n",
    "    #alternative method to erosion - erosion followed by dilation \n",
    "    #opening = cv2.morphologyEx(th0, cv2.MORPH_OPEN, kernel, iterations = 2)\n",
    "    #opening = cv2.erode(opening, kernel, iterations = 1)\n",
    "    \n",
    "    # Finding the contours for each method - this is for comparison; choose the one that best delineates the gastruloids\n",
    "    (_, contours, _) = cv2.findContours(erosion, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    (_, contours2, _) = cv2.findContours(erosion, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    (_, contours3, _) = cv2.findContours(im_out_0, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    (_, contours4, _) = cv2.findContours(th0, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    contours2 = sorted(contours2, key = cv2.contourArea, reverse = True)[:1]\n",
    "    contours3 = sorted(contours3, key = cv2.contourArea, reverse = True)[:1]\n",
    "    contours = sorted(contours, key = cv2.contourArea, reverse = True)[:1]\n",
    "    \n",
    "    if not contours2: \n",
    "        contours2 = np.array([0])\n",
    "    contours_0rough.append(contours2)  \n",
    "    \n",
    "    if not contours3: \n",
    "        contours3 = np.array([0])\n",
    "    smoothed_0.append(contours3)\n",
    "    \n",
    "    # ---- add smoothing code excerpt here ------\n",
    "    # -------------------------------------------\n",
    "    \n",
    "    v3 = v.copy()\n",
    "    cv2.drawContours(v3, contours2, -1, (255, 0, 0), 10)     # draw contours over original image\n",
    "    cv2.drawContours(v2, contours3, -1, (255, 0, 0), 10)     # draw contours over original image\n",
    "    #cv2.drawContours(v, contours4, -1, (255, 0, 0), 10)     # draw contours over original image\n",
    "\n",
    "    #plot it \n",
    "    fig, ax = plt.subplots(1, 6, figsize = (16,5))\n",
    "    ax[0].imshow(th0)\n",
    "    ax[0].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[0].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[0].set_title(\"Before Floodfilling\")\n",
    "    \n",
    "    ax[1].imshow(im_out_0)\n",
    "    ax[1].set_xlabel(\"Distance / Pixels\")\n",
    "    #ax[1].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[1].set_title(\"After Floodfilling\")\n",
    "    \n",
    "    ax[2].imshow(erosion) \n",
    "    ax[2].set_xlabel(\"Distance / Pixels\")\n",
    "    #ax[2].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[2].set_title(\"After Floodfilling & Erosion \")\n",
    "    \n",
    "    ax[3].imshow(v)\n",
    "    ax[3].set_xlabel(\"Distance / Pixels\")\n",
    "    #ax[3].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[3].set_title(\"No Floodfill contour\")\n",
    "    \n",
    "    ax[4].imshow(v2)\n",
    "    ax[4].set_xlabel(\"Distance / Pixels\")\n",
    "    #ax[4].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[4].set_title(\"Floodfill contour\")\n",
    "\n",
    "    ax[5].imshow(v3)\n",
    "    ax[5].set_xlabel(\"Distance / Pixels\")\n",
    "    #ax[4].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[5].set_title(\"Floodfill & Erosion contour\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "#save_locn = \"/Users/alexandrabaranowski/Desktop/Images for report /\"\n",
    "#fig.savefig(save_locn+'1. 2015BraGFP 72h Empty.png')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "################# this is for gastruloids that have unclear borders (e.g., dead cells) and so normal contouring \n",
    "#doesn't capture the edges well ###########\n",
    "# adapted from: https://agniva.me/scipy/2016/10/25/contour-smoothing.html\n",
    "# if used, ADD IT TO THE ABOVE CODE at the point of ------- add smoothing code excerpt here ---------\n",
    "smoothed_0 = []\n",
    "for i, v in enumerate(erosions):\n",
    "    v2 = v.copy()\n",
    "    (_, contours, _) = cv2.findContours(v, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key = cv2.contourArea, reverse = True)[:1]\n",
    "    cv2.drawContours(v3, contours2, -1, (255, 0, 0), 10)     # draw contours over original image\n",
    "    \n",
    "    for contour in contours:\n",
    "        x,y = contour.T\n",
    "        # Convert from numpy arrays to normal arrays\n",
    "        x = x.tolist()[0]\n",
    "        y = y.tolist()[0]\n",
    "        \n",
    "        # https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.splprep.html\n",
    "        tck, u = splprep([x,y], u=None, s=1.0, per=1)\n",
    "        \n",
    "        # https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linspace.html\n",
    "        u_new = np.linspace(u.min(), u.max(), 25)\n",
    "       \n",
    "        # https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.splev.html\n",
    "        x_new, y_new = splev(u_new, tck, der=0)\n",
    "        \n",
    "        # Convert it back to numpy format for opencv to be able to display it\n",
    "        res_array = [[[int(i[0]), int(i[1])]] for i in zip(x_new,y_new)]\n",
    "        \n",
    "        smoothed.append(np.asarray(res_array, dtype=np.int32))\n",
    "            \n",
    "        # Draw the smoothed contours on the original image\n",
    "        cv2.drawContours(v2,smoothed, -1, (255, 0, 0), 10)\n",
    "        if not smoothed: \n",
    "            smoothed = np.array([0])\n",
    "        smoothed_0.append(smoothed)\n",
    "                \n",
    "        fig, ax = plt.subplots(1, 2, figsize = (10,3))\n",
    "        ax[0].imshow(v)\n",
    "        ax[0].imshow(v2)\n",
    "        \n",
    "        #msk = cv2.bitwise_and(blurred_copy,blurred_copy, mask = contours)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'erosions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f000f1092da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mareas_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mareas0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# this will only be used for QC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merosions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#find areas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilledareas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountNonZero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'erosions' is not defined"
     ]
    }
   ],
   "source": [
    "areas_0 = []\n",
    "areas0 = [] # this will only be used for QC\n",
    "for i, v in enumerate(erosions):\n",
    "    #find areas\n",
    "    filledareas = cv2.countNonZero(v)\n",
    "    areas0.append(filledareas)\n",
    "\n",
    "\n",
    "for i, c in enumerate(smoothed_0):\n",
    "    if np.size(c)>1:\n",
    "        cnt = c[0]\n",
    "        area = cv2.contourArea(cnt)\n",
    "        areas_0.append(area)\n",
    "    else:\n",
    "        areas_0.append(0)\n",
    "\n",
    "print areas_0\n",
    "print areas0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find different shape attributes of the empty channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline \n",
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "perimeters_0 = []\n",
    "circularity_0 = []\n",
    "minc_centres_0 = []\n",
    "minc_radii_0 = []\n",
    "min_ellipse_0 = []\n",
    "boxes_0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,c) in enumerate(contours_0rough): # input is the list of contours, depending on which was used above \n",
    "    \n",
    "    if np.size(c)>1:\n",
    "        cnnt = c[0]\n",
    "        # perimeters\n",
    "        perimeters = cv2.arcLength(cnnt,True)\n",
    "        perimeters_0.append(perimeters)\n",
    "\n",
    "        #circularity\n",
    "        circularity = ((4*np.pi*areas_0[i])/perimeters**2)\n",
    "        circularity_0.append(circularity)\n",
    "\n",
    "        #minimum enclosing circle, radius and centre of the circle are used as parameters \n",
    "        (x,y),radius = cv2.minEnclosingCircle(cnnt)\n",
    "        centre = (int(x),int(y))\n",
    "        radius = int(radius)\n",
    "        #cv2.circle(image_copies_0[i],centre,radius,(0,255,0),2)\n",
    "        minc_centres_0.append(centre)\n",
    "        minc_radii_0.append(radius)\n",
    "\n",
    "        # fitting minimum rotated rectangle\n",
    "        rect = cv2.minAreaRect(cnnt)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "        boxes_0.append(box)\n",
    "        #box_contour = cv2.drawContours(v,[box],0,(0,0,255),2)\n",
    "    else:\n",
    "        #min_ellipse_0.append(0)\n",
    "        perimeters_0.append(0)\n",
    "        circularity_0.append(0)\n",
    "        minc_centres_0.append(0)\n",
    "        minc_radii_0.append(0)\n",
    "        boxes_0.append(0)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating moments and aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments_0 = []\n",
    "hu_moments_0 = []\n",
    "\n",
    "cx_0 = [] # the x co-ordinate of the centroid\n",
    "cy_0 = [] # the y co-ordinate of the cetnroid\n",
    "for i, v in enumerate(contours_0rough):\n",
    "    cnt = v[0]\n",
    "    M = cv2.moments(cnt)\n",
    "    \n",
    "    #Centroid\n",
    "    cx = int(M['m10']/M['m00'])\n",
    "    cy = int(M['m01']/M['m00'])\n",
    "    \n",
    "    #Calculate the Hu moments - these are scale, rotation and translation invariant so these will be used \n",
    "    hu = cv2.HuMoments(M).flatten()\n",
    "    \n",
    "    hu_moments_0.append(hu)\n",
    "    moments_0.append(M)   \n",
    "    cx_0.append(cx)\n",
    "    cy_0.append(cy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect ratios normalised for orientation of the gastruloids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_ratios_0 = []\n",
    "\n",
    "width = []\n",
    "height = []\n",
    "\n",
    "for i, v in enumerate(contours_0rough):\n",
    "    cnt = v[0]    \n",
    "    #Find the aspect ratio, which is the ratio of the width to height of bounding rectangle of the contour\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    width.append(w)\n",
    "    height.append(h)\n",
    "    \n",
    "for i, v in enumerate(width): # normalise the AR so that all values are > 1 so the actual distance can be measured\n",
    "    k = height[i]\n",
    "    #print k\n",
    "    if v > k:\n",
    "        aspect_ratios_0.append(np.float(v) / k)\n",
    "    else:\n",
    "        aspect_ratios_0.append(np.float(k) / v)\n",
    "    \n",
    "\n",
    "print aspect_ratios_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the 7 Hu moments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Hu moments are a way to look at the shape - if an image is rotated but is the same image, then they are \n",
    "#expected to be the same; it is used a lot for object DETECTION \n",
    "\n",
    "# These values are proved to be invariants to the image scale, rotation, \n",
    "#and reflection except the seventh one, whose sign is changed by reflection.\n",
    "# https://docs.opencv.org/3.0-beta/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html\n",
    "\n",
    "I1 = []; I2 = []; I3 = []; I4 = []; I5 = []; I6 = []; I7 = []\n",
    "\n",
    "for i in range(0,len(hu_moments_0)):\n",
    "    moment = hu_moments_0[i]\n",
    "    for j in range(0,2): \n",
    "        i1 = moment[0]\n",
    "        i2 = moment[1]\n",
    "        i3 = moment[2]\n",
    "        i4 = moment[3]\n",
    "        i5 = moment[4]\n",
    "        i6 = moment[5]\n",
    "        i7 = moment[6]\n",
    "        #print moment\n",
    "    \n",
    "    I1.append(i1); I2.append(i2); I3.append(i3); I4.append(i4); I5.append(i5); I6.append(i6); I7.append(i7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-transform the Hu moments - this might not always be necessary but in some cases will show the features better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first the array with all 7 together \n",
    "log_hu = []\n",
    "for i, v in enumerate(hu_moments_0):\n",
    "    log_hu_i = -np.sign(v) * np.log10(np.abs(v))\n",
    "    log_hu.append(log_hu_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also for each separate moment \n",
    "log_i1 = []; log_i2=[]; log_i3=[]; log_i4=[]; log_i5=[]; log_i6=[]; log_i7=[]\n",
    "I=[I1, I2, I3, I4, I5, I6, I7]\n",
    "\n",
    "for j in range(0,7):\n",
    "    for i,v in enumerate(I[j]):\n",
    "        log_hu_i = -np.sign(v) * np.log10(np.abs(v))\n",
    "        if j == 0:\n",
    "            log_i1.append(log_hu_i)\n",
    "        if j == 1:\n",
    "            log_i2.append(log_hu_i)\n",
    "        if j == 2:\n",
    "            log_i3.append(log_hu_i)\n",
    "        if j == 3: \n",
    "            log_i4.append(log_hu_i)\n",
    "        if j == 4:\n",
    "            log_i5.append(log_hu_i)\n",
    "        if j == 5:\n",
    "            log_i6.append(log_hu_i)\n",
    "        elif j ==6 :\n",
    "            log_i7.append(log_hu_i)\n",
    "\n",
    "log_i = [log_i1, log_i2, log_i3, log_i4, log_i5, log_i6, log_i7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a mask to get the Haralick features from inside the Gastruloid only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_masks_0 = []\n",
    "for i, v in enumerate(image_copies_0):  \n",
    "    img = v.copy()\n",
    "    \n",
    "    #Gaussian Blur - same as that used to de-noise when making the original Ch0 masks \n",
    "    blur = cv2.GaussianBlur(image_copy,(5,5),0)\n",
    "    \n",
    "    # Create a mask using the area of the gastruloid found in the empty channel \n",
    "    blurred_masked_img = cv2.bitwise_and(blur,blur, mask = erosions[i])\n",
    "    blurred_masks_0.append(blurred_masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://gogul09.github.io/software/image-classification-python\n",
    "### Get Haralick features - texture / more global than intensity values ###\n",
    "from mahotas import features\n",
    "\n",
    "haralick_features = []\n",
    "\n",
    "for i, v in enumerate(blurred_masks_0): \n",
    "    #img = cv2.GaussianBlur(v, (3,3), 0)\n",
    "    img = v.copy()\n",
    "    haralick = features.haralick(img).mean(axis=0)\n",
    "    haralick_features.append(haralick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each into its own variable\n",
    "H1ang2ndmoment = []; H2contrast = []; H3correlation = []; H4sumsqvar = []; H5invdiffmoment = []\n",
    "H6sumavg = []; H7sumvar = []; H8sumentropy = []; H9entropy = []; H10diffvar = []; H11diffentropy = []\n",
    "H12imcorr1 = []; H13imcorr2 = []\n",
    "\n",
    "\n",
    "for i in range(0,len(haralick_features)):\n",
    "    f = haralick_features[i]\n",
    "    for j in range(0,2):\n",
    "        H1 = f[0]\n",
    "        H2 = f[1]\n",
    "        H3 = f[2]\n",
    "        H4 = f[3]\n",
    "        H5 = f[4]\n",
    "        H6 = f[5]\n",
    "        H7 = f[6]\n",
    "        H8 = f[7]\n",
    "        H9 = f[8]\n",
    "        H10 = f[9]\n",
    "        H11 = f[10]\n",
    "        H12 = f[11]\n",
    "        H13 = f[12]\n",
    "        \n",
    "    H1ang2ndmoment.append(H1); H2contrast.append(H2); H3correlation.append(H3); H4sumsqvar.append(H4); \n",
    "    H5invdiffmoment.append(H5); H6sumavg.append(H6); H7sumvar.append(H7)\n",
    "    H8sumentropy.append(H8); H9entropy.append(H9); H10diffvar.append(H10); H11diffentropy.append(H11); H12imcorr1.append(H12); \n",
    "    H13imcorr2.append(H13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing - convexity hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull_areas_0 = []\n",
    "hull_0 = []\n",
    "convexity_defects_0 = []\n",
    "\n",
    "for (i,c) in enumerate(contours_0rough): \n",
    "    cnt = c[0]\n",
    "\n",
    "    #find the hull for for the main contour \n",
    "    hull = cv2.convexHull(cnt,returnPoints = False) # this is used for the convexity defects, but is not kept\n",
    "    hull2 = cv2.convexHull(cnt,returnPoints = True)\n",
    "    hull_0.append(hull2)\n",
    "    \n",
    "    #Calculating the hull area \n",
    "    hullarea = cv2.contourArea(hull2)\n",
    "    hull_areas_0.append(hullarea)   \n",
    "\n",
    "    #convexity defects \n",
    "    defects = cv2.convexityDefects(cnt,hull)\n",
    "    convexity_defects_0.append(defects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating solidity / convexity - i.e., gld area / area of the convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convexity_0 = []\n",
    "for i, v in enumerate(areas_0):\n",
    "    convexity = v / hull_areas_0[i]\n",
    "    #print convexity\n",
    "    convexity_0.append(convexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the ellipse: keep major and minor axis lengths as these don't change by rotation, but the angle does (so not saving that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_ellipse_axis = [] # length of the major axis of the ellipse \n",
    "minor_ellipse_axis = []\n",
    "for i, c in enumerate(contours_0rough):\n",
    "    if np.size(c) > 5:\n",
    "        cnt = c[0]\n",
    "        (x,y),(MA,ma),angle = cv2.fitEllipse(cnt)\n",
    "        #print x,y, MA, ma, angle\n",
    "        major_ellipse_axis.append(ma)\n",
    "        minor_ellipse_axis.append(MA)\n",
    "        \n",
    "    else:\n",
    "        major_ellipse_axis.append(0)\n",
    "        minor_ellipse_axis.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Ensuring all lists are of equal length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"1) Area list:\", len(areas_0)\n",
    "print \"2) Perimeter list:\", len(perimeters_0)\n",
    "print \"3) Circularity list:\", len(circularity_0)\n",
    "print \"4) Hull list:\", len(hull_0)\n",
    "print \"5) Convexity defects list:\", len(convexity_defects_0)\n",
    "print \"6) Hull area list:\", len(hull_areas_0)\n",
    "print \"7) Minimum enclosing circle radii list:\", len(minc_radii_0)\n",
    "print \"8) Minimum enclosing rotated rectangle list:\", len(boxes_0)\n",
    "\n",
    "print \"\\n9) Cy list:\", len(cy_0)\n",
    "print \"10) Cx list:\", len(cx_0)\n",
    "print \"11) Moments list:\", len(moments_0)\n",
    "print \"12) Hu Moments list:\", len(hu_moments_0)\n",
    "print \"13) Hu-I1:\", len(I1),',', \"14) Hu-I2:\", len(I2),',', \"15) Hu-I3:\", len(I3),',', \"16) Hu-I4:\", len(I4),',', \"17) Hu-I5:\", len(I5),',',\\\n",
    "\"18) Hu-I6:\", len(I6),',', \"19) Hu-I7:\", len(I7)\n",
    "print \"20) Log Hu Moments list:\", len(log_hu)\n",
    "print \"21) LogHu-I1:\", len(log_i1),',', \"22) LogHu-I2:\", len(log_i2),',', \"23) LogHu-I3:\", len(log_i3),',', \"24) LogHu-I4:\", len(log_i4),',', \"25) LogHu-I5:\", len(log_i5),',',\\\n",
    "\"26) LogHu-I6:\", len(log_i6),',', \"27) LogHu-I7:\", len(log_i7)\n",
    "\n",
    "print \"\\n28) H1ang2ndmoment:\", len(H1ang2ndmoment), ',', \"29) H2contrast:\", len(H2contrast), ',', \"30) H3correlation:\" len(H3correlation),\\\n",
    "',', \"31) H4sumsqvar:\", len(H4sumsqvar), ',', \"32) H5invdiffmoment:\", len(H5invdiffmoment), ',', \"33) H6sumavg:\", len(H6sumavg), ',',\\\n",
    "\"34) H7sumvar:\", len(H7sumvar), ',', \"35) H8sumentropy:\", len(H8sumentropy), ',', \"36) H9entropy:\", len(H9entropy), ',',\\\n",
    "\"37) H10diffvar:\", len(H10diffvar), ',', \"38) H11diffentropy:\", len(H11diffentropy), ',', \"39) H12imcorr1:\", len(H12imcorr1), ',',\\\n",
    "\"40) H13imcorr2:\", len(H13imcorr2)\n",
    "\n",
    "\n",
    "print \"\\n41) AR list:\", len(aspect_ratios_0)\n",
    "print \"42) Convexity / Solidity list:\", len(convexity_0)\n",
    "print \"43) Major Ellipse list:\", len(major_ellipse_axis)\n",
    "print \"44) Minor Ellipse list:\", len(minor_ellipse_axis)\n",
    "\n",
    "if len(areas_0) == len(perimeters_0) == len(circularity_0) == len(hull_0) == len(convexity_defects_0) \\\n",
    "==len(hull_areas_0) == len(minc_radii_0) == len(boxes_0) == len(moments_0) == len(cy_0) == len(cx_0) == len(hu_moments_0)\\\n",
    "== len(I1) ==len(I2) ==len(I3) ==len(I4) ==len(I5)==len(I6)==len(I7)==len(log_hu) ==len(log_i1)==len(log_i2) \\\n",
    "==len(log_i3) ==len(log_i4) ==len(log_i5) ==len(log_i6) ==len(log_i7) == len(H1ang2ndmoment) == len(H2contrast) \\\n",
    "len(H3correlation) ==len(H4sumsqvar)==len(H5invdiffmoment) ==len(H6sumavg)==len(H7sumvar) ==len(H8sumentropy) \\\n",
    "==len(H9entropy) ==len(H10diffvar)== len(H11diffentropy) ==len(H12imcorr1)==len(H13imcorr2) \\\n",
    "== len(aspect_ratios_0) == len(convexity_0) == len(major_ellipse_axis) ==len(minor_ellipse_axis) :\n",
    "    print \"\\nCheck complete - All lists are of same length\"\n",
    "else: \n",
    "    print \"\\nError: all list lengths are not equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluorescent Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ch1 (here Sox2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) De-noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline \n",
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "blurred_1 = []\n",
    "image_copies_1 = []\n",
    "blurred_masks_1 = []\n",
    "\n",
    "for i, v in enumerate(files_list[1][0]):  \n",
    "    # readthe original image\n",
    "    image_1 = np.array(Image.open(files_list[1][0][i]))\n",
    "    #image_colour0 = np.array(Image.open(files_list_full[0][0][i]))\n",
    "    \n",
    "    #print image_1.shape\n",
    "    \n",
    "    #Make a copy bc all the transformations change the image, and we don't want to change the original\n",
    "    image_copy = image_1.copy()\n",
    "    img_copy = image_1.copy()\n",
    "    image_copy1 = image_1.copy()\n",
    "    image_copies_1.append(image_copy)\n",
    "    \n",
    "    #Gaussian Blur \n",
    "    blur = cv2.GaussianBlur(image_copy,(5,5),0) \n",
    "    blurred_1.append(blur)\n",
    "    \n",
    "    # Create a mask using the area of the gastruloid found in the empty channel \n",
    "    blurred_masked_img = cv2.bitwise_and(blur,blur, mask = erosions[i])\n",
    "    blurred_masks_1.append(blurred_masked_img)\n",
    "\n",
    "    #fig, ax = plt.subplots(1, 3, figsize = (10,3))\n",
    "    #ax[0].imshow(image_1)\n",
    "    #ax[1].imshow(blur) \n",
    "    #ax[2].imshow(blurred_masked_img)\n",
    "    \n",
    "    #ax[2].imshow(masked_img)\n",
    "    #ax[3].imshow(th1)\n",
    "    #ax[1].imshow(blurred_masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(blurred_1):\n",
    "\n",
    "    ## finding the range of the grayscale values so we can do the thresholding properly \n",
    "    # https://mmeysenburg.github.io/image-processing/05-creating-histograms/ \n",
    "    histogram = cv2.calcHist([v], [0], None, [256], [0, 256])\n",
    "    plt.plot(histogram)\n",
    "\n",
    "    plt.title(\"Grayscale Histogram\")\n",
    "    plt.xlabel(\"Grayscale Value\")\n",
    "    plt.ylabel(\"Pixels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "im_out_1 = []\n",
    "contours_1 = []\n",
    "masks_1 = []\n",
    "th_in_use1 = []\n",
    "\n",
    "for i, v in enumerate(blurred_masks_1):\n",
    "    ret, th1 = cv2.threshold(v, 160, 255, cv2.THRESH_BINARY)\n",
    "    mask_inv = cv2.bitwise_not(v)\n",
    "    th_in_use1.append(th1)\n",
    "\n",
    "    im_floodfill_1 = th1.copy()\n",
    "    \n",
    "    # Mask used for flood filling; Notice the size needs to be 2 pixels larger than the image.\n",
    "    h, w, = th1.shape[:2]\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "\n",
    "    # Floodfill from point (0, 0)\n",
    "    cv2.floodFill(im_floodfill_1, mask, (0,0), 255)\n",
    "\n",
    "    # Invert floodfilled image\n",
    "    im_floodfill_inv_1 = cv2.bitwise_not(im_floodfill_1)\n",
    "\n",
    "    # Combine the two images to get the foreground.\n",
    "    im_out_1s = th1 | im_floodfill_inv_1\n",
    "\n",
    "    im_out_1.append(im_out_1s)\n",
    "    \n",
    "    (_, contours, _) = cv2.findContours(th1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # If only the largest contour is required, add [:1] at the end of the expression; when expression is speckled, \n",
    "    # measure all contours to count the number of speckles \n",
    "    contours = sorted(contours, key = cv2.contourArea, reverse = True)[:1]\n",
    "   \n",
    "    cv2.drawContours(v, contours, -1, (255, 0, 0), 10)     # draw contours over original image\n",
    "    if not contours:\n",
    "        contours = np.array([0])\n",
    "    contours_1.append(contours)\n",
    "    \n",
    "    #create mask \n",
    "    masked_img = cv2.bitwise_and(v,v, mask = im_out_1s)\n",
    "    masks_1.append(masked_img)\n",
    "    \n",
    "    #plot it \n",
    "    fig, ax = plt.subplots(1, 3, figsize = (13,3))\n",
    "    ax[0].imshow(th1)\n",
    "    ax[0].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[0].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[0].set_title(\"Mask before floodilling\")\n",
    "    ax[1].imshow(im_out_1s)\n",
    "    ax[1].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[1].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[1].set_title(\"Mask After Floodfilling\")\n",
    "    ax[2].imshow(v) \n",
    "    ax[2].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[2].set_ylabel(\"Distance / Pixels\")\n",
    "    ax[2].set_title(\"Contour drawn around Fluorescence \\nArea In Gastruloid Mask\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #save_locn = \"/Users/alexandrabaranowski/Desktop/Images for report /\"\n",
    "    #fig.savefig(save_locn+'1. 2015BraGFP 72h Ch1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas1 = [] # this will be used to see the total ch1 fluo\n",
    "for i, v in enumerate(im_out_1):\n",
    "    #find areas\n",
    "    filledareas = cv2.countNonZero(v)\n",
    "    areas1.append(filledareas)\n",
    "\n",
    "areas_1=[] # this will be used to find the area of the biggest contour, so to calculate the perimeter, circ etc \n",
    "for i, c in enumerate(contours_1):\n",
    "    if np.size(c)>1:\n",
    "        cnt = c[0]\n",
    "        area = cv2.contourArea(cnt)\n",
    "        areas_1.append(area)\n",
    "    else:\n",
    "        areas_1.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "perimeters_1 = []\n",
    "circularity_1 = []\n",
    "minc_centres_1 = []\n",
    "minc_radii_1 = []\n",
    "min_ellipse_1 = []\n",
    "boxes_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,c) in enumerate(contours_1): \n",
    "    \n",
    "    if np.size(c)>3:\n",
    "        cnt = c[0]\n",
    "\n",
    "        # perimeters\n",
    "        perimeters = cv2.arcLength(cnt,True)\n",
    "        perimeters_1.append(perimeters)\n",
    "\n",
    "        #circularity\n",
    "        circularity = ((4*np.pi*areas_1[i])/perimeters**2)\n",
    "        circularity_1.append(circularity)\n",
    "\n",
    "        #minimum enclosing circle, radius and centre of the circle are used as parameters \n",
    "        (x,y),radius = cv2.minEnclosingCircle(cnt)\n",
    "        centre = (int(x),int(y))\n",
    "        radius = int(radius)\n",
    "        #cv2.circle(image_copies_0[i],centre,radius,(0,255,0),2)\n",
    "        minc_centres_1.append(centre)\n",
    "        minc_radii_1.append(radius)\n",
    "\n",
    "        # fitting minimum rotated rectangle\n",
    "        rect = cv2.minAreaRect(cnt)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "        boxes_1.append(box)\n",
    "    else:\n",
    "        perimeters_1.append(0)\n",
    "        circularity_1.append(0)\n",
    "        minc_centres_1.append(0)\n",
    "        minc_radii_1.append(0)\n",
    "        #min_ellipse_1.append(0)\n",
    "        boxes_1.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating moments and aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments_1 = []\n",
    "hu_moments_1 = []\n",
    "\n",
    "cx_1 = [] # the x co-ordinate of the centroid\n",
    "cy_1 = [] # the y co-ordinate of the cetnroid\n",
    "for i, v in enumerate(contours_1):\n",
    "    cnt = v[0]\n",
    "    M = cv2.moments(cnt)\n",
    "    \n",
    "    #Calculate the Hu moments \n",
    "    hu = cv2.HuMoments(M).flatten() \n",
    "    hu_moments_1.append(hu)\n",
    "    \n",
    "    try:\n",
    "        #Centroid\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "     \n",
    "        moments_1.append(M)   \n",
    "        cx_1.append(cx)\n",
    "        cy_1.append(cy)\n",
    "        \n",
    "    except ZeroDivisionError:\n",
    "        #hu_moments_1.append(0)\n",
    "        moments_1.append(0)   \n",
    "        cx_1.append(0)\n",
    "        cy_1.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the 7 Hu moments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I1_1 = []; I2_1 = []; I3_1 = []; I4_1 = []; I5_1 = []; I6_1 = []; I7_1 = []\n",
    "\n",
    "for i in range(0,len(hu_moments_1)):\n",
    "    moment = hu_moments_1[i]\n",
    "    for j in range(0,2): \n",
    "        i1 = moment[0]\n",
    "        i2 = moment[1]\n",
    "        i3 = moment[2]\n",
    "        i4 = moment[3]\n",
    "        i5 = moment[4]\n",
    "        i6 = moment[5]\n",
    "        i7 = moment[6]\n",
    "    \n",
    "    I1_1.append(i1); I2_1.append(i2); I3_1.append(i3); I4_1.append(i4); I5_1.append(i5); I6_1.append(i6); I7_1.append(i7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-transform the Hu moments - this might not always be necessary but in some cases will show the features better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first the array with all 7 together \n",
    "log_hu1 = []\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "for i, v in enumerate(hu_moments_1):\n",
    "    try: \n",
    "        log_hu_i = -np.sign(v) * np.log10(np.abs(v))\n",
    "        log_hu1.append(log_hu_i)\n",
    "    except v==0:\n",
    "        log_hu1.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also for each separate moment \n",
    "log_i11 = []; log_i21=[]; log_i31=[]; log_i41=[]; log_i51=[]; log_i61=[]; log_i71=[]\n",
    "I_1=[I1_1, I2_1, I3_1, I4_1, I5_1, I6_1, I7_1]\n",
    "\n",
    "for j in range(0,7):\n",
    "    for i,v in enumerate(I_1[j]):\n",
    "        log_hu_i = -np.sign(v) * np.log10(np.abs(v))\n",
    "        if j == 0:\n",
    "            log_i11.append(log_hu_i)\n",
    "        if j == 1:\n",
    "            log_i21.append(log_hu_i)\n",
    "        if j == 2:\n",
    "            log_i31.append(log_hu_i)\n",
    "        if j == 3: \n",
    "            log_i41.append(log_hu_i)\n",
    "        if j == 4:\n",
    "            log_i51.append(log_hu_i)\n",
    "        if j == 5:\n",
    "            log_i61.append(log_hu_i)\n",
    "        elif j ==6 :\n",
    "            log_i71.append(log_hu_i)\n",
    "\n",
    "log_ich1 = [log_i11, log_i21, log_i31, log_i41, log_i51, log_i61, log_i71]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hull & Convexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull_areas_1 = []\n",
    "hull_1 = []\n",
    "convexity_defects_1 = []\n",
    "\n",
    "for (i,c) in enumerate(contours_1): \n",
    "    if np.size(c)>1:\n",
    "        cnt = c[0]\n",
    "\n",
    "    #find the hull for for the main contour \n",
    "        hull = cv2.convexHull(cnt,returnPoints = False) # this is used for the convexity defects, but is not kept\n",
    "        hull2 = cv2.convexHull(cnt,returnPoints = True)\n",
    "        hull_1.append(hull2)\n",
    "    \n",
    "        #Calculating the hull area \n",
    "        hullarea = cv2.contourArea(hull2)\n",
    "        hull_areas_1.append(hullarea)   \n",
    "\n",
    "        #convexity defects \n",
    "        defects = cv2.convexityDefects(cnt,hull)\n",
    "        convexity_defects_1.append(defects)\n",
    "    \n",
    "    else:\n",
    "        hull_1.append(0)\n",
    "        hull_areas_1.append(0)\n",
    "        convexity_defects_1.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convexity / Solidity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convexity_1 = []\n",
    "for i, v in enumerate(areas_1):\n",
    "    try:\n",
    "        convexity = v / hull_areas_1[i]\n",
    "    #print convexity\n",
    "        convexity_1.append(convexity)\n",
    "    except ZeroDivisionError:\n",
    "        convexity_1.append(0)\n",
    "print len(convexity_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major and Minor ellipses of the fluo area - don't need this if expression isn't homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_ratios_1 = []\n",
    "width1 = []\n",
    "height1 = []\n",
    "\n",
    "for i, c in enumerate(contours_1):\n",
    "    if np.size(c) >1:\n",
    "        cnt = c[0]\n",
    "        #Find the aspect ratio, which is the ratio of the width to height of bounding rectangle of the contour\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        width1.append(w)\n",
    "        height1.append(h)\n",
    "    else:\n",
    "        width1.append(0)\n",
    "        height1.append(0)\n",
    "    \n",
    "for i, v in enumerate(width1):\n",
    "    k = height1[i]\n",
    "    #print k\n",
    "    try: \n",
    "        if v > k:\n",
    "            aspect_ratios_1.append(np.float(v) / k)\n",
    "        else:\n",
    "            aspect_ratios_1.append(np.float(k) / v)\n",
    "    except ZeroDivisionError:\n",
    "        aspect_ratios_1.append(0)\n",
    "    \n",
    "\n",
    "print len(aspect_ratios_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_ellipse_axis1 = [] # length of the major axis of the ellipse \n",
    "minor_ellipse_axis1 = []\n",
    "\n",
    "for i, c in enumerate(contours_1):\n",
    "    if np.size(c) >5:\n",
    "        cnt = c[0]\n",
    "        (x,y),(MA,ma),angle = cv2.fitEllipse(cnt)\n",
    "        #print x,y, MA, ma, angle\n",
    "        major_ellipse_axis1.append(ma)\n",
    "        minor_ellipse_axis1.append(MA)\n",
    "        \n",
    "    else:\n",
    "        major_ellipse_axis1.append(0)\n",
    "        minor_ellipse_axis1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC for list length against each other and empty channel area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"45) contour Area list:\", len(areas_1)\n",
    "print \"46) full Area list:\", len(areas1)\n",
    "\n",
    "print \"47) Perimeter list:\", len(perimeters_1)\n",
    "print \"48) Circularity list:\", len(circularity_1)\n",
    "print \"49) Hull list:\", len(hull_1)\n",
    "print \"50) Convexity defects list:\", len(convexity_defects_1)\n",
    "print \"51) Hull area list:\", len(hull_areas_1)\n",
    "print \"52) Minimum enclosing circle radii list:\", len(minc_radii_1)\n",
    "print \"53) Minimum enclosing rotated rectangle list:\", len(boxes_1)\n",
    "\n",
    "print \"\\n54) Cy list:\", len(cy_1)\n",
    "print \"55) Cx list:\", len(cx_1)\n",
    "print \"56) Moments list:\", len(moments_1)\n",
    "print \"57) Hu Moments list:\", len(hu_moments_1)\n",
    "print \"58) Hu-I1:\", len(I1_1),',', \"59) Hu-I2:\", len(I2_1),',', \"60) Hu-I3:\", len(I3_1),',', \"61) Hu-I4:\", len(I4_1),',', \"62) Hu-I5:\", len(I5_1),',',\\\n",
    "\"63) Hu-I6:\", len(I6_1),',', \"64) Hu-I7:\", len(I7_1)\n",
    "print \"65) Log Hu Moments list:\", len(log_hu1)\n",
    "print \"66) LogHu-I1:\", len(log_i11),',', \"67) LogHu-I2:\", len(log_i21),',', \"68) LogHu-I3:\", len(log_i31),',', \"69) LogHu-I4:\", len(log_i41),',', \"70) LogHu-I5:\", len(log_i51),',',\\\n",
    "\"71) LogHu-I6:\", len(log_i61),',', \"72) LogHu-I7:\", len(log_i71)\n",
    "\n",
    "print \"\\n73) AR list:\", len(aspect_ratios_1)\n",
    "print \"74) Convexity / Solidity list:\", len(convexity_1)\n",
    "print \"75) Major Ellipse list:\", len(major_ellipse_axis1)\n",
    "print \"76) Minor Ellipse list:\", len(minor_ellipse_axis1)\n",
    "\n",
    "if len(areas_1) == len(areas1) == len(perimeters_1) == len(circularity_1) == len(hull_1) == len(convexity_defects_1) \\\n",
    "== len(hull_areas_1) == len(minc_radii_1) == len(boxes_1) == len(moments_1) == len(cy_1) == len(cx_1)== len(hu_moments_1) \\\n",
    "== len(I1_1) ==len(I2_1) ==len(I3_1) ==len(I4_1) ==len(I5_1)==len(I6_1)==len(I7_1)==len(log_hu1) ==len(log_i11)==len(log_i21) \\\n",
    "==len(log_i31) ==len(log_i41) ==len(log_i51) ==len(log_i61) ==len(log_i71) == len(aspect_ratios_1) == len(convexity_1) \\\n",
    "==len(major_ellipse_axis1) == len(minor_ellipse_axis1) == len(areas_0):\n",
    "    print \"\\nCheck complete - All lists are of same length\"\n",
    "else: \n",
    "    print \"\\nError: all list lengths are not equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ch2 (Sox17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) De-noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "blurred_2 = []\n",
    "image_copies_2 = []\n",
    "blurred_masks_2 = []\n",
    "blurred_masks_2noisy = []\n",
    "for i, v in enumerate(files_list[2][0]):\n",
    "    # readthe original image\n",
    "    image_2 = np.array(Image.open(files_list[2][0][i]))\n",
    "    #image_colour0 = np.array(Image.open(files_list_full[0][0][i]))\n",
    "    \n",
    "    #Make a copy bc all the transformations change the image, and we don't want to change the original\n",
    "    image_copy = image_2.copy()\n",
    "    image_copy2 = image_2.copy()\n",
    "    image_copies_2.append(image_copy)\n",
    "    \n",
    "    #Gaussian Blur \n",
    "    blur = cv2.GaussianBlur(image_copy,(15,15),0) \n",
    "    blur2 = cv2.medianBlur(image_copy2, 5)\n",
    "    blurred_2.append(blur2)\n",
    "    \n",
    "    blurred_masked_img = cv2.bitwise_and(blur,blur, mask = erosions[i])\n",
    "    blurred_masks_2.append(blurred_masked_img)\n",
    "    \n",
    "    blurred_mask_noisy = cv2.bitwise_and(image_copy,image_copy, mask = threshold_0[i])\n",
    "    blurred_masks_2noisy.append(blurred_mask_noisy)\n",
    "\n",
    "    #fig, ax = plt.subplots(1, 3, figsize = (10,3))\n",
    "    #ax[0].imshow(image_2)\n",
    "    #ax[1].imshow(blur) \n",
    "    #ax[2].imshow(blur2)\n",
    "    #ax[3].imshow(blurred_masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(blurred_masks_2):\n",
    "\n",
    "    ## finding the range of the grayscale values so we can do the thresholding properly \n",
    "    # https://mmeysenburg.github.io/image-processing/05-creating-histograms/ \n",
    "    histogram = cv2.calcHist([v], [0], None, [256], [0, 256])\n",
    "    plt.plot(histogram)\n",
    "\n",
    "    plt.title(\"Grayscale Histogram\")\n",
    "    plt.xlabel(\"Grayscale Value\")\n",
    "    plt.ylabel(\"Pixels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Thresholding - make sure you fix the thresholds!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "im_out_2 = []\n",
    "contours_2 = []\n",
    "masks_2 = []\n",
    "th2_in_use = []\n",
    "\n",
    "for i, v in enumerate(blurred_masks_2):\n",
    "    ret, th2 = cv2.threshold(v, 150, 255, cv2.THRESH_BINARY)\n",
    "    mask_inv = cv2.bitwise_not(v)\n",
    "    th2_in_use.append(th2)\n",
    "    \n",
    "    #further removes noise \n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    opening = cv2.morphologyEx(th2,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
    "\n",
    "    im_floodfill_2 = th2.copy()\n",
    "    \n",
    "    # Mask used for flood filling; Notice the size needs to be 2 pixels larger than the image.\n",
    "    h, w, = th2.shape[:2]\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "\n",
    "    # Floodfill from point (0, 0)\n",
    "    cv2.floodFill(im_floodfill_2, mask, (0,0), 255)\n",
    "\n",
    "    # Invert floodfilled image\n",
    "    im_floodfill_inv_2 = cv2.bitwise_not(im_floodfill_2)\n",
    "\n",
    "    # Combine the two images to get the foreground.\n",
    "    im_out_2s = th2 | im_floodfill_inv_2\n",
    "\n",
    "    im_out_2.append(im_out_2s)\n",
    "    \n",
    "    # try thresholding using the median \n",
    "    median_blur = cv2.medianBlur(image_copies_2[i], 9)\n",
    "    denoised = v - median_blur \n",
    "    ret, th = cv2.threshold(median_blur, 245, 254, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    image = th + th2\n",
    "    \n",
    "    #create mask \n",
    "    masked_img = cv2.bitwise_and(v,v, mask = im_out_1s)\n",
    "    masks_2.append(masked_img)\n",
    "    \n",
    "    #stat = ImageStat.Stat(v, mask=masked_img)\n",
    "    #print stat.mean, stat.var, stat.median\n",
    "    \n",
    "    #plot it \n",
    "    fig, ax = plt.subplots(1, 3, figsize = (10,3))\n",
    "    ax[0].imshow(th2) # probably this?? \n",
    "    ax[1].imshow(im_out_2s)\n",
    "    ax[2].imshow(v)\n",
    "    #ax[3].imshow(th)\n",
    "    #ax[3].imshow(median_blur)\n",
    "#ax[5].imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total area of gastruloid covered by Ch2 fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas2 = []\n",
    "for i, v in enumerate(th2_in_use):\n",
    "    #find areas\n",
    "    filledareas = cv2.countNonZero(v)\n",
    "    areas2.append(filledareas)\n",
    "\n",
    "print areas2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Measuring fluorescence intensity of Ch2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"77) Area list:\", len(areas2)\n",
    "\n",
    "if len(areas2) == len(areas_0):\n",
    "    print \"Check complete: all ok\"\n",
    "else: \n",
    "    print \"List length not correct - check \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ch3 (T/Bra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) De-noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_3 = []\n",
    "image_copies_3 = []\n",
    "blurred_masks_3 = []\n",
    "\n",
    "for i, v in enumerate(files_list[3][0]):\n",
    "    # readthe original image\n",
    "    image_3 = np.array(Image.open(files_list[3][0][i]))\n",
    "    #image_colour0 = np.array(Image.open(files_list_full[0][0][i]))\n",
    "    \n",
    "    #Make a copy bc all the transformations change the image, and we don't want to change the original\n",
    "    image_copy = image_3.copy()\n",
    "    image_copy2 = image_3.copy()\n",
    "    image_copies_3.append(image_copy)\n",
    "    \n",
    "    #Gaussian Blur \n",
    "    blur = cv2.GaussianBlur(image_copy,(15,15),0)\n",
    "    blur2 = cv2.medianBlur(image_copy2, 5)\n",
    "    blurred_3.append(blur)\n",
    "    \n",
    "    blurred_masked_img = cv2.bitwise_and(blur,blur, mask = erosions[i])\n",
    "    blurred_masks_3.append(blurred_masked_img)\n",
    "\n",
    "    #fig, ax = plt.subplots(1, 3, figsize = (10,3))\n",
    "    #ax[0].imshow(image_3)\n",
    "    #ax[1].imshow(blur) \n",
    "    #ax[2].imshow(blur2)\n",
    "    #ax[2].imshow(blurred_masked_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(blurred_masks_3):\n",
    "\n",
    "    ## finding the range of the grayscale values so we can do the thresholding properly \n",
    "    # https://mmeysenburg.github.io/image-processing/05-creating-histograms/ \n",
    "    histogram = cv2.calcHist([v], [0], None, [256], [0, 256])\n",
    "    plt.plot(histogram)\n",
    "\n",
    "    plt.title(\"Grayscale Histogram\")\n",
    "    plt.xlabel(\"Grayscale Value\")\n",
    "    plt.ylabel(\"Pixels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Thresholding - Make sure you fix the thresholds and have a reason for all of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_out_3 = []\n",
    "contours_3 = []\n",
    "masks_3 = []\n",
    "th_in_use_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "\n",
    "for i, v in enumerate(blurred_masks_3):\n",
    "    ret, thch3 = cv2.threshold(v, 150, 255, cv2.THRESH_BINARY)\n",
    "    mask_inv = cv2.bitwise_not(v)\n",
    "    th_in_use_3.append(thch3)\n",
    "    \n",
    "    #further removes noise - use this if needed \n",
    "    #kernel = np.ones((5,5),np.uint8)\n",
    "    #opening = cv2.morphologyEx(thch3,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
    "\n",
    "    im_floodfill_3 = thch3.copy()\n",
    "    \n",
    "    # Mask used for flood filling; Notice the size needs to be 2 pixels larger than the image.\n",
    "    h, w, = thch3.shape[:2]\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "\n",
    "    # Floodfill from point (0, 0)\n",
    "    cv2.floodFill(im_floodfill_3, mask, (0,0), 255)\n",
    "\n",
    "    # Invert floodfilled image\n",
    "    im_floodfill_inv_3 = cv2.bitwise_not(im_floodfill_3)\n",
    "\n",
    "    # Combine the two images to get the foreground.\n",
    "    im_out_3s = thch3 | im_floodfill_inv_3\n",
    "\n",
    "    im_out_3.append(im_out_3s)\n",
    "    \n",
    "    (_, contours, _) = cv2.findContours(thch3, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key = cv2.contourArea, reverse = True)[:1]\n",
    "     \n",
    "    cv2.drawContours(v, contours, -1, (255, 0, 0), 10)     # draw contours over original image\n",
    "    if not contours:\n",
    "        contours = np.array([0])\n",
    "    contours_3.append(contours)\n",
    "        \n",
    "    #create mask \n",
    "    masked_img = cv2.bitwise_and(v,v, mask = im_out_1s)\n",
    "    masks_3.append(masked_img)\n",
    "    \n",
    "    #stat = ImageStat.Stat(v, mask=masked_img)\n",
    "    #print stat.mean, stat.var, stat.median\n",
    "    \n",
    "    #plot it \n",
    "    fig, ax = plt.subplots(1, 3, figsize = (10,3))\n",
    "    ax[0].imshow(thch3)\n",
    "    ax[1].imshow(im_out_3s) # this is the best one here \n",
    "    ax[2].imshow(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Areas - Proportion of gld covered by Ch3 fluo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas3 = []\n",
    "for i, v in enumerate(th_in_use_3):\n",
    "    #find areas\n",
    "    filledareas = cv2.countNonZero(v)\n",
    "    areas3.append(filledareas)\n",
    "\n",
    "areas_3= []\n",
    "for i, c in enumerate(contours_3):\n",
    "    if np.size(c)>1:\n",
    "        cnt = c[0]\n",
    "        area = cv2.contourArea(cnt)\n",
    "        areas_3.append(area)\n",
    "    else:\n",
    "        areas_3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "perimeters_3 = []\n",
    "circularity_3 = []\n",
    "minc_centres_3 = []\n",
    "minc_radii_3 = []\n",
    "min_ellipse_3 = []\n",
    "boxes_3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,c) in enumerate(contours_3): \n",
    "    if np.size(c)>1:\n",
    "        cnt = c[0]\n",
    "\n",
    "        # perimeters\n",
    "        perimeters = cv2.arcLength(cnt,True)\n",
    "        perimeters_3.append(perimeters)\n",
    "\n",
    "        #circularity\n",
    "        circularity = ((4*np.pi*areas_3[i])/perimeters**2)\n",
    "        circularity_3.append(circularity)\n",
    "\n",
    "        #minimum enclosing circle, radius and centre of the circle are used as parameters \n",
    "        (x,y),radius = cv2.minEnclosingCircle(cnt)\n",
    "        centre = (int(x),int(y))\n",
    "        radius = int(radius)\n",
    "        #cv2.circle(image_copies_0[i],centre,radius,(0,255,0),2)\n",
    "        minc_centres_3.append(centre)\n",
    "        minc_radii_3.append(radius)\n",
    "\n",
    "        # fitting minimum rotated rectangle\n",
    "        rect = cv2.minAreaRect(cnt)\n",
    "        box = cv2.boxPoints(rect)\n",
    "        box = np.int0(box)\n",
    "        boxes_3.append(box)\n",
    "    \n",
    "    else: \n",
    "        perimeters_3.append(0)\n",
    "        circularity_3.append(0)\n",
    "        minc_centres_3.append(0)\n",
    "        minc_radii_3.append(0)\n",
    "        boxes_3.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments_3 = []\n",
    "hu_moments_3 = []\n",
    "\n",
    "cx_3 = [] # the x co-ordinate of the centroid\n",
    "cy_3 = [] # the y co-ordinate of the cetnroid\n",
    "for i, v in enumerate(contours_3):\n",
    "    cnt = v[0]\n",
    "    M = cv2.moments(cnt)\n",
    "    \n",
    "    #Calculate the Hu moments \n",
    "    hu = cv2.HuMoments(M).flatten() \n",
    "    hu_moments_3.append(hu)\n",
    "    \n",
    "    try:\n",
    "        #Centroid\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "\n",
    "        moments_3.append(M)   \n",
    "        cx_3.append(cx)\n",
    "        cy_3.append(cy)\n",
    "        \n",
    "    except ZeroDivisionError: \n",
    "        cy_3.append(0)\n",
    "        cx_3.append(0)\n",
    "        moments_3.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the 7 Hu moments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I1_3 = []; I2_3 = []; I3_3 = []; I4_3 = []; I5_3 = []; I6_3 = []; I7_3 = []\n",
    "\n",
    "for i in range(0,len(hu_moments_3)):\n",
    "    moment = hu_moments_3[i]\n",
    "    for j in range(0,2): \n",
    "        i1 = moment[0]\n",
    "        i2 = moment[1]\n",
    "        i3 = moment[2]\n",
    "        i4 = moment[3]\n",
    "        i5 = moment[4]\n",
    "        i6 = moment[5]\n",
    "        i7 = moment[6]\n",
    "        #print moment\n",
    "    \n",
    "    I1_3.append(i1); I2_3.append(i2); I3_3.append(i3); I4_3.append(i4); I5_3.append(i5); I6_3.append(i6); I7_3.append(i7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-transform the Hu moments - this might not always be necessary but in some cases will show the features better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first the array with all 7 together \n",
    "log_hu3 = []\n",
    "for i, v in enumerate(hu_moments_3):\n",
    "    log_hu_i = -np.sign(v) * np.log10(np.abs(v))\n",
    "    log_hu3.append(log_hu_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also for each separate moment \n",
    "log_i13 = []; log_i23=[]; log_i33=[]; log_i43=[]; log_i53=[]; log_i63=[]; log_i73=[]\n",
    "I_3=[I1_3, I2_3, I3_3, I4_3, I5_3, I6_3, I7_3]\n",
    "\n",
    "for j in range(0,7):\n",
    "    for i,v in enumerate(I_3[j]):\n",
    "        log_hu_i = -np.sign(v) * np.log10(np.abs(v))\n",
    "        if j == 0:\n",
    "            log_i13.append(log_hu_i)\n",
    "        if j == 1:\n",
    "            log_i23.append(log_hu_i)\n",
    "        if j == 2:\n",
    "            log_i33.append(log_hu_i)\n",
    "        if j == 3: \n",
    "            log_i43.append(log_hu_i)\n",
    "        if j == 4:\n",
    "            log_i53.append(log_hu_i)\n",
    "        if j == 5:\n",
    "            log_i63.append(log_hu_i)\n",
    "        elif j ==6 :\n",
    "            log_i73.append(log_hu_i)\n",
    "\n",
    "log_ich3 = [log_i13, log_i23, log_i33, log_i43, log_i53, log_i63, log_i73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARs; Post-processing - hulls & hull area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hull_areas_3 = []\n",
    "hull_3 = []\n",
    "convexity_defects_3 = []\n",
    "aspect_ratios_3 = []\n",
    "width3 = []\n",
    "height3 = []\n",
    "\n",
    "for (i,c) in enumerate(contours_3): \n",
    "    if np.size(c)>1: \n",
    "        cnt = c[0]\n",
    "\n",
    "        #find the hull for for the main contour \n",
    "        hull = cv2.convexHull(cnt,returnPoints = False) # this is used for the convexity defects, but is not kept\n",
    "        hull2 = cv2.convexHull(cnt,returnPoints = True)\n",
    "        hull_3.append(hull2)\n",
    "\n",
    "        #Calculating the hull area \n",
    "        hullarea = cv2.contourArea(hull2)\n",
    "        hull_areas_3.append(hullarea)   \n",
    "\n",
    "        #convexity defects \n",
    "        defects = cv2.convexityDefects(cnt,hull)\n",
    "        convexity_defects_3.append(defects)\n",
    "\n",
    "        #AR\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        width3.append(w)\n",
    "        height3.append(h)\n",
    "        \n",
    "    else: \n",
    "        hull_areas_3.append(0)\n",
    "        aspect_ratios_3.append(0)\n",
    "        hull_3.append(0)\n",
    "        convexity_defects_3.append(0)\n",
    "        \n",
    "for i, v in enumerate(width3):\n",
    "    k = height3[i]\n",
    "    #print k\n",
    "    if v > k:\n",
    "        aspect_ratios_3.append(np.float(v) / k)\n",
    "    else:\n",
    "        aspect_ratios_3.append(np.float(k) / v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convexity / solidity - change the area used for the other ones too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convexity_3 = []\n",
    "for i, v in enumerate(areas_3):\n",
    "    try: \n",
    "        convexity = v / hull_areas_3[i]\n",
    "        #print convexity\n",
    "        convexity_3.append(convexity)\n",
    "    except ZeroDivisionError:\n",
    "        convexity_3.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major & minor ellipse axes - check if this is correct or if they are the other way round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_ellipse_axis3 = [] # length of the major axis of the ellipse \n",
    "minor_ellipse_axis3 = []\n",
    "\n",
    "for i, c in enumerate(contours_3):\n",
    "    if np.size(c)>1:\n",
    "        cnt = c[0]\n",
    "        (x,y),(MA,ma),angle = cv2.fitEllipse(cnt)\n",
    "        #print x,y, MA, ma, angle\n",
    "        major_ellipse_axis3.append(ma)\n",
    "        minor_ellipse_axis3.append(MA)\n",
    "    else: \n",
    "        major_ellipse_axis3.append(0)\n",
    "        minor_ellipse_axis3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"78) contour Area list:\", len(areas_3)\n",
    "print \"79) full Area list:\", len(areas3)\n",
    "print \"80) Perimeter list:\", len(perimeters_3)\n",
    "print \"81) Circularity list:\", len(circularity_3)\n",
    "print \"82) Hull list:\", len(hull_3)\n",
    "print \"83) Convexity defects list:\", len(convexity_defects_3)\n",
    "print \"84) Hull area list:\", len(hull_areas_3)\n",
    "print \"85) Minimum enclosing circle radii list:\", len(minc_radii_3)\n",
    "print \"86) Minimum enclosing rotated rectangle list:\", len(boxes_3)\n",
    "\n",
    "print \"\\n87) Cy list:\", len(cy_3)\n",
    "print \"88) Cx list:\", len(cx_3)\n",
    "print \"89) Moments list:\", len(moments_3)\n",
    "print \"90) Hu Moments list:\", len(hu_moments_3)\n",
    "print \"91) Hu-I1:\", len(I1_3),',', \"92) Hu-I2:\", len(I2_3),',', \"93) Hu-I3:\", len(I3_3),',', \"94) Hu-I4:\", len(I4_3),',', \"95) Hu-I5:\", len(I5_3),',',\\\n",
    "\"96) Hu-I6:\", len(I6_3),',', \"97) Hu-I7:\", len(I7_3)\n",
    "print \"98) Log Hu Moments list:\", len(log_hu3)\n",
    "print \"99) LogHu-I1:\", len(log_i13),',', \"100) LogHu-I2:\", len(log_i23),',', \"101) LogHu-I3:\", len(log_i33),',', \"102) LogHu-I4:\", len(log_i43),',', \"103) LogHu-I5:\", len(log_i53),',',\\\n",
    "\"104) LogHu-I6:\", len(log_i63),',', \"105) LogHu-I7:\", len(log_i73)\n",
    "\n",
    "print \"\\n106) AR list:\", len(aspect_ratios_3)\n",
    "print \"107) Convexity / Solidity list:\", len(convexity_3)\n",
    "print \"108) Major Ellipse list:\", len(major_ellipse_axis3)\n",
    "print \"109) Minor Ellipse list:\", len(minor_ellipse_axis3)\n",
    "\n",
    "if len(areas_3) == len(perimeters_3) == len(circularity_3) == len(hull_3) == len(convexity_defects_3) \\\n",
    "==len(hull_areas_3) == len(minc_radii_3) == len(boxes_3) == len(moments_3) == len(cy_3) == len(cx_3) ==len(hu_moments_3) \\\n",
    "== len(I1_3) ==len(I2_3) ==len(I3_3) ==len(I4_3) ==len(I5_3)==len(I6_3)==len(I7_3)==len(log_hu3) ==len(log_i13)==len(log_i23) \\\n",
    "==len(log_i33) ==len(log_i43) ==len(log_i53) ==len(log_i63) ==len(log_i73) == len(aspect_ratios_3) == len(convexity_3) \\\n",
    "==len(major_ellipse_axis3) == len(minor_ellipse_axis3) == len(areas_0):\n",
    "    print \"\\nCheck complete - All lists are of same length\"\n",
    "else: \n",
    "    print \"\\nError: all list lengths are not equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Fluorescence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Illumination and Background Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all the bg images \n",
    "\n",
    "bg_1 = []\n",
    "bg_2 = []\n",
    "bg_3 = []\n",
    "for i, v in enumerate(bg_list[1][0]):\n",
    "    bg1 = np.array(Image.open(bg_list[1][0][i]))\n",
    "    bg_1.append(bg1)\n",
    "for i, v in enumerate(bg_list[2][0]):\n",
    "    bg2 = np.array(Image.open(bg_list[2][0][i]))\n",
    "    bg_2.append(bg2)\n",
    "\n",
    "for i, v in enumerate(bg_list[3][0]):    \n",
    "    bg3 = np.array(Image.open(bg_list[3][0][i]))\n",
    "    bg_3.append(bg3)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correcting for uneven illumination - channel 1\n",
    "\n",
    "% matplotlib inline \n",
    "fgs_roi1 = []\n",
    "fgs_subtract1 = []\n",
    "fgs_norm1 = []\n",
    "\n",
    "for i, v in enumerate(files_list_full[1][0]):\n",
    "    image = np.array(Image.open(files_list_full[1][0][i]))\n",
    "    bg1 = bg_1[0]\n",
    "    \n",
    "    img = image.copy()  \n",
    "    img = cv2.GaussianBlur(img, (5,5),0)\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    #extract pixel values for the 16-bit image \n",
    "    WIDTH, HEIGHT = img.size\n",
    "    pix = img.load()\n",
    "    data = np.asarray(img.getdata()) # getdata returns the contents of the image as a sequence object containing #pixel values \n",
    "    data = data.reshape((HEIGHT,WIDTH)) #reshape the data to the actual dimensions\n",
    "    \n",
    "    # extract pixel values for the full image bg \n",
    "    bgimg = bg1.copy()\n",
    "    bgimg = cv2.GaussianBlur(bgimg, (5,5), 0)\n",
    "    bgimg = Image.fromarray(bgimg)\n",
    "    WIDTHb, HEIGHTb = bgimg.size\n",
    "    pixb = bgimg.load()\n",
    "\n",
    "    datab = np.asarray(bgimg.getdata()) \n",
    "    datab = datab.reshape((HEIGHTb,WIDTHb)) \n",
    "    \n",
    "    # extract an ROI to calculate and subtract the background \n",
    "    roi = np.array(img.copy())\n",
    "    roi = roi[0:64, 0:1024]\n",
    "    roi = Image.fromarray(roi)\n",
    "\n",
    "    WIDTHr, HEIGHTr = roi.size\n",
    "    pixr = roi.load()\n",
    "    datar = np.asarray(roi.getdata())\n",
    "    datar = datar.reshape((HEIGHTr,WIDTHr))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    #returns the column-wise average of the array elements along the axis for each of the types \n",
    "    reduced_data = data.mean(axis=0) \n",
    "    \n",
    "    reduced_datar = datar.mean(axis=0) \n",
    "    \n",
    "    reduced_datab = datab.mean(axis=0) \n",
    "\n",
    "    fg = reduced_data - reduced_datar\n",
    "    fgs_roi1.append(fg)\n",
    "    \n",
    "    fg6 = reduced_data - reduced_datab\n",
    "    fgs_subtract1.append(fg6) \n",
    "    \n",
    "    bg = np.matrix(bgimg)\n",
    "    fgg = np.matrix(img)\n",
    "\n",
    "    meanbg = np.mean(bgimg)\n",
    "    meanfg = np.mean(img)\n",
    "    nbg = bg * meanfg\n",
    "    nfg = fgg * meanbg \n",
    "    result = nfg - nbg\n",
    "    result = result - np.min(result)\n",
    "    result = result / np.max(result)\n",
    "    result2 = result.copy()\n",
    "    #result2 = cv2.GaussianBlur(result2, (15,15), 0)\n",
    "    \n",
    "    result2 = Image.fromarray(result2)\n",
    "    WIDTHre, HEIGHTre = result2.size\n",
    "    pixre = result2.load()\n",
    "    datare = np.asarray(result2.getdata()) \n",
    "    datare = datare.reshape((HEIGHTre,WIDTHre))\n",
    "    reduced_datare = datare.mean(axis=0) \n",
    "    fgs_norm1.append(reduced_datare)\n",
    "    \n",
    "    \n",
    "    fig,ax = plt.subplots(1, 2, figsize = (13,3), sharex=False, sharey = False)\n",
    "\n",
    "    ax[0].plot(fg6)\n",
    "    ax[0].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[0].set_ylabel(\"Grayscale Value\")\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].plot(fg)\n",
    "    ax[1].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[1].set_ylabel(\"Grayscale Value\")\n",
    "    ax[1].set_title(\"Normalised from ROI\")\n",
    "    #ax[2].plot(fg6)\n",
    "    #ax[2].set_xlabel(\"Distance / Pixels\")\n",
    "    #ax[2].set_ylabel(\"Grayscale Value\")\n",
    "    #ax[2].set_title(\"Normalised from Empty Image\")\n",
    "    #ax[3].plot(reduced_datare)\n",
    "    #ax[3].set_xlabel(\"Distance / Pixels\")\n",
    "    #ax[3].set_ylabel(\"Grayscale Value\")\n",
    "    #ax[3].set_title(\"Normalised from Empty Image\")\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "   # plt.show()\n",
    "    \n",
    "    #save_locn = \"/Users/alexandrabaranowski/Desktop/Images for report /\"\n",
    "    #fig.savefig(save_locn+'Ch1 Background subtraction ROI vs Empty Image.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to see which method to use; if fine, the preferable method is roi_subtract from the empty image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking whether the two methods are similar enough to be able to use the empty channel \n",
    "#matplotlib.style.use('ggplot')\n",
    "\n",
    "# Pearson correlation for the two methods\n",
    "Pearson = stats.pearsonr(fgs_subtract1[0], fgs_norm1[0])\n",
    "print 'Correlation Coefficient:', Pearson[0]\n",
    "print 'p-value:', Pearson[1]\n",
    "\n",
    "\n",
    "plt.scatter(fgs_subtract1[0], fgs_roi1[0], color='c')\n",
    "plt.xlabel('De-noised Fluorescence from Empty Image / Pixel Intensity')\n",
    "plt.ylabel('De-noised & Normalised Fluorescence from \\nEmpty Image / Pixel Intensity');\n",
    "plt.title('Correlation of Background Subtraction Methods')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#Finding correlation\n",
    "dfcorr = pd.DataFrame({'Empty': fgs_subtract1[0]})\n",
    "dfcorr['Normalised'] = dfcorr['Empty'] + fgs_roi1[0] # positively correlated with 'a'\n",
    "\n",
    "dfcorr.corr()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "save_locn = \"/Users/alexandrabaranowski/Desktop/Images for report /\"\n",
    "#fig.savefig(save_locn+'2016-03-15 Correlation of Background subtraction Methods.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fgs_subtract1[0]\n",
    "y = fgs_norm1[0]\n",
    "\n",
    "def r2(x1, y1):\n",
    "    return np.float(stats.pearsonr(x, y)[0] ** 2)\n",
    "sns.jointplot(x, y, kind=\"reg\", stat_func = r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correcting for uneven illumination - channel 2\n",
    "\n",
    "% matplotlib inline \n",
    "fgs_roi2 = []\n",
    "fgs_subtract2 = []\n",
    "fgs_norm2 = []\n",
    "\n",
    "for i, v in enumerate(files_list_full[2][0]):\n",
    "    image = np.array(Image.open(files_list_full[2][0][i]))\n",
    "    bg2 = bg_2[0]\n",
    "    img = image.copy()  \n",
    "    img = cv2.GaussianBlur(img, (15,15),0)\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    #extract pixel values for the 16-bit image \n",
    "    WIDTH, HEIGHT = img.size\n",
    "    pix = img.load()\n",
    "    data = np.asarray(img.getdata()) # getdata returns the contents of the image as a sequence object containing #pixel values \n",
    "    data = data.reshape((HEIGHT,WIDTH)) #reshape the data to the actual dimensions\n",
    "    \n",
    "    # extract pixel values for the full image bg \n",
    "    bgimg = bg2.copy()\n",
    "    bgimg = cv2.GaussianBlur(bgimg, (15,15), 0)\n",
    "    bgimg = Image.fromarray(bgimg)\n",
    "    WIDTHb, HEIGHTb = bgimg.size\n",
    "    pixb = bgimg.load()\n",
    "\n",
    "    datab = np.asarray(bgimg.getdata()) \n",
    "    datab = datab.reshape((HEIGHTb,WIDTHb)) \n",
    "    \n",
    "    # extract an ROI to calculate and subtract the background \n",
    "    roi = np.array(img.copy())\n",
    "    roi = roi[0:64, 0:1024]\n",
    "    roi = Image.fromarray(roi)\n",
    "\n",
    "    WIDTHr, HEIGHTr = roi.size\n",
    "    pixr = roi.load()\n",
    "    datar = np.asarray(roi.getdata())\n",
    "    datar = datar.reshape((HEIGHTr,WIDTHr))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    #returns the column-wise average of the array elements along the axis for each of the types \n",
    "    reduced_data = data.mean(axis=0) \n",
    "    \n",
    "    reduced_datar = datar.mean(axis=0) \n",
    "    \n",
    "    reduced_datab = datab.mean(axis=0) \n",
    "\n",
    "    fg = reduced_data - reduced_datar\n",
    "    fgs_roi2.append(fg)\n",
    "    \n",
    "    fg6 = reduced_data - reduced_datab\n",
    "    fgs_subtract2.append(fg6) \n",
    "    \n",
    "    bg = np.matrix(bgimg)\n",
    "    fgg = np.matrix(img)\n",
    "\n",
    "    meanbg = np.mean(bgimg)\n",
    "    meanfg = np.mean(img)\n",
    "    nbg = bg * meanfg\n",
    "    nfg = fgg * meanbg \n",
    "    result = nfg - nbg\n",
    "    result = result - np.min(result)\n",
    "    result = result / np.max(result)\n",
    "    result2 = result.copy()\n",
    "    #result2 = cv2.GaussianBlur(result2, (15,15), 0)\n",
    "    \n",
    "    result2 = Image.fromarray(result2)\n",
    "    WIDTHre, HEIGHTre = result2.size\n",
    "    pixre = result2.load()\n",
    "    datare = np.asarray(result2.getdata()) \n",
    "    datare = datare.reshape((HEIGHTre,WIDTHre))\n",
    "    reduced_datare = datare.mean(axis=0) \n",
    "    fgs_norm2.append(reduced_datare)\n",
    "    \n",
    "    \n",
    "    fig,ax = plt.subplots(1, 4, figsize = (13,3), sharex=False, sharey = False)\n",
    "\n",
    "    ax[0].plot(reduced_data)\n",
    "    ax[0].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[0].set_ylabel(\"Grayscale Value\")\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].plot(fg)\n",
    "    ax[1].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[1].set_ylabel(\"Grayscale Value\")\n",
    "    ax[1].set_title(\"Normalised from ROI\")\n",
    "    ax[2].plot(fg6)\n",
    "    ax[2].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[2].set_ylabel(\"Grayscale Value\")\n",
    "    ax[2].set_title(\"Normalised from Empty Image\")\n",
    "    ax[3].plot(reduced_datare)\n",
    "    ax[3].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[3].set_ylabel(\"Grayscale Value\")\n",
    "    ax[3].set_title(\"Normalised from Empty Image\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    #save_locn = \"/Users/alexandrabaranowski/Desktop/Images for report /\"\n",
    "    #fig.savefig(save_locn+'Ch1 Background subtraction ROI vs Empty Image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correcting for uneven illumination - channel 3\n",
    "\n",
    "% matplotlib inline \n",
    "fgs_roi3 = []\n",
    "fgs_subtract3 = []\n",
    "fgs_norm3 = []\n",
    "\n",
    "for i, v in enumerate(files_list_full[3][0]):\n",
    "    image = np.array(Image.open(files_list_full[3][0][i]))\n",
    "    bg3 = bg_3[0]\n",
    "    img = image.copy()  \n",
    "    img = cv2.GaussianBlur(img, (15,15),0)\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    #extract pixel values for the 16-bit image \n",
    "    WIDTH, HEIGHT = img.size\n",
    "    pix = img.load()\n",
    "    data = np.asarray(img.getdata()) # getdata returns the contents of the image as a sequence object containing #pixel values \n",
    "    data = data.reshape((HEIGHT,WIDTH)) #reshape the data to the actual dimensions\n",
    "    \n",
    "    # extract pixel values for the full image bg \n",
    "    bgimg = bg3.copy()\n",
    "    bgimg = cv2.GaussianBlur(bgimg, (15,15), 0)\n",
    "    bgimg = Image.fromarray(bgimg)\n",
    "    WIDTHb, HEIGHTb = bgimg.size\n",
    "    pixb = bgimg.load()\n",
    "\n",
    "    datab = np.asarray(bgimg.getdata()) \n",
    "    datab = datab.reshape((HEIGHTb,WIDTHb)) \n",
    "    \n",
    "    # extract an ROI to calculate and subtract the background \n",
    "    roi = np.array(img.copy())\n",
    "    roi = roi[0:64, 0:1024]\n",
    "    roi = Image.fromarray(roi)\n",
    "\n",
    "    WIDTHr, HEIGHTr = roi.size\n",
    "    pixr = roi.load()\n",
    "    datar = np.asarray(roi.getdata())\n",
    "    datar = datar.reshape((HEIGHTr,WIDTHr))\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    #returns the column-wise average of the array elements along the axis for each of the types \n",
    "    reduced_data = data.mean(axis=0) \n",
    "    \n",
    "    reduced_datar = datar.mean(axis=0) \n",
    "    \n",
    "    reduced_datab = datab.mean(axis=0) \n",
    "\n",
    "    fg = reduced_data - reduced_datar\n",
    "    fgs_roi3.append(fg)\n",
    "    \n",
    "    fg6 = reduced_data - reduced_datab\n",
    "    fgs_subtract3.append(fg6) \n",
    "    \n",
    "    bg = np.matrix(bgimg)\n",
    "    fgg = np.matrix(img)\n",
    "\n",
    "    meanbg = np.mean(bgimg)\n",
    "    meanfg = np.mean(img)\n",
    "    nbg = bg * meanfg\n",
    "    nfg = fgg * meanbg \n",
    "    result = nfg - nbg\n",
    "    result = result - np.min(result)\n",
    "    result = result / np.max(result)\n",
    "    result2 = result.copy()\n",
    "    #result2 = cv2.GaussianBlur(result2, (15,15), 0)\n",
    "    \n",
    "    result2 = Image.fromarray(result2)\n",
    "    WIDTHre, HEIGHTre = result2.size\n",
    "    pixre = result2.load()\n",
    "    datare = np.asarray(result2.getdata()) \n",
    "    datare = datare.reshape((HEIGHTre,WIDTHre))\n",
    "    reduced_datare = datare.mean(axis=0) \n",
    "    fgs_norm3.append(reduced_datare)\n",
    "    \n",
    "    \n",
    "    fig,ax = plt.subplots(1, 2, figsize = (13,3), sharex=False, sharey = False)\n",
    "\n",
    "    ax[0].plot(fg6)\n",
    "    ax[0].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[0].set_ylabel(\"Grayscale Value\")\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].plot(fg)\n",
    "    ax[1].set_xlabel(\"Distance / Pixels\")\n",
    "    ax[1].set_ylabel(\"Grayscale Value\")\n",
    "    ax[1].set_title(\"Normalised from ROI\")\n",
    "   # ax[2].plot(fg6)\n",
    "   # ax[2].set_xlabel(\"Distance / Pixels\")\n",
    "   # ax[2].set_ylabel(\"Grayscale Value\")\n",
    "   # ax[2].set_title(\"Normalised from Empty Image\")\n",
    "   # ax[3].plot(reduced_datare)\n",
    "   # ax[3].set_xlabel(\"Distance / Pixels\")\n",
    "   # ax[3].set_ylabel(\"Grayscale Value\")\n",
    "   # ax[3].set_title(\"Normalised from Empty Image\")\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "   #plt.show()\n",
    "    \n",
    "    #save_locn = \"/Users/alexandrabaranowski/Desktop/Images for report /\"\n",
    "    #fig.savefig(save_locn+'Ch1 Background subtraction ROI vs Empty Image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the fluorescence values and normalising the minimum to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxfluo1 = []; maxfluo2 = []; maxfluo3 = [] # this is a list of the max fluorescence value for EACH gastruloid \n",
    "meanfluo1 = []; meanfluo2 = []; meanfluo3 = [] # This is a list of the mean fluorescence value for EACH gastruloid\n",
    "stdevfluo1 = []; stdevfluo2 = []; stdevfluo3 = []\n",
    "minfluo1 = []; minfluo2 = []; minfluo3 = []\n",
    "\n",
    "mins1 = []\n",
    "for i, v in enumerate(fgs_subtract1):\n",
    "    mins1.append(min(v))\n",
    "\n",
    "# Make sure that the lowest value for fluorescence is zero; bc the same image is used for background subtraction, it \n",
    "# might lead to small deviations / values slightly below zero - this shifts up all values by the minimum \n",
    "a = min(mins1)\n",
    "for i, v in enumerate(fgs_subtract1):\n",
    "    if a < 0: \n",
    "        v = v - a\n",
    "    else: \n",
    "        v = v\n",
    "    maxfluo1.append(v.max())\n",
    "    stdevfluo1.append(np.std(v))\n",
    "    meanfluo1.append(v.mean())\n",
    "    minfluo1.append(v.min())\n",
    "    \n",
    "avg_fluo_1 = np.mean(maxfluo1)\n",
    "stddev_fluo_1 = np.std(maxfluo1, axis=0)# use the one without the outliers to calculate the avg max fluo\n",
    "\n",
    "mins2 = []\n",
    "for i, v in enumerate(fgs_subtract2):\n",
    "    mins2.append(min(v))\n",
    "\n",
    "b = min(mins2)\n",
    "for i, v in enumerate(fgs_subtract2):\n",
    "    if b < 0: \n",
    "        v = v - b\n",
    "    else: \n",
    "        v = v\n",
    "    maxfluo2.append(v.max())\n",
    "    stdevfluo2.append(np.std(v))\n",
    "    meanfluo2.append(v.mean())\n",
    "    minfluo2.append(v.min())\n",
    "\n",
    "avg_fluo_2 = np.mean(maxfluo2)\n",
    "stddev_fluo_2 = np.std(maxfluo2, axis=0)\n",
    "\n",
    "# Ch3 \n",
    "mins3 = []\n",
    "for i, v in enumerate(fgs_subtract3):\n",
    "    mins3.append(min(v))\n",
    "\n",
    "c = min(mins3)\n",
    "for i, v in enumerate(fgs_subtract3):\n",
    "    if c < 0: \n",
    "        v = v - c\n",
    "    else: \n",
    "        v = v\n",
    "    maxfluo3.append(v.max())\n",
    "    stdevfluo3.append(np.std(v))\n",
    "    meanfluo3.append(v.mean())\n",
    "    minfluo3.append(v.min())\n",
    "\n",
    "avg_fluo_3 = np.mean(maxfluo3)\n",
    "stddev_fluo_3 = np.std(maxfluo3, axis=0)\n",
    "\n",
    "print 'Average Max value across all Ch1 gastruloids is:', avg_fluo_1\n",
    "print 'Average Max value across all Ch2 gastruloids is:', avg_fluo_2\n",
    "print 'Average Max value across all Ch3 gastruloids is:', avg_fluo_3\n",
    "print '\\nSt Dev of max fluo across Ch1 gastruloids is', stddev_fluo_1\n",
    "print 'St Dev of max fluo across Ch2 gastruloids is', stddev_fluo_2\n",
    "print 'St Dev of max fluo across Ch3 gastruloids is', stddev_fluo_3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC based on area and circularity of the empty channel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Field-of-view quality control - removing images with no Gastruloids, debris etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: Using area and circularity, but many different methods have been tested below, and can be added or removed as required to get the best option; manual verification against the ch0 contours is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_area = []\n",
    "\n",
    "print 'Number of observations before removing outliers is %d' %len(areas_0)\n",
    "#print areas_0\n",
    "areas = np.array(areas_0)\n",
    "\n",
    "mean = np.mean(areas, axis=0)\n",
    "sd = np.std(areas, axis=0)\n",
    "bins = 50 \n",
    "new_area = [x for x in areas_0 if (x > mean - 1.5 * sd)]\n",
    "new_area = [x for x in new_area if (x < mean + 1.5 * sd)]\n",
    "\n",
    "#print 'Min area is %d' %min(new_area)\n",
    "#print 'Max area is %d' %max(new_area)\n",
    "#print 'Avg area is %d'%(sum(new_area)/len(new_area))\n",
    "\n",
    "circ = np.array(circularity_0)\n",
    "\n",
    "mean = np.mean(circ, axis=0)\n",
    "sd = np.std(circ, axis=0)\n",
    "bins = 50 \n",
    "new_circ = [x for x in circularity_0 if (x > mean -1.5 * sd)]\n",
    "new_circ = [x for x in new_circ if (x < mean + 2 * sd)]\n",
    "\n",
    "\n",
    "# print new_area\n",
    "print 'Number of observations within the threshold is %d' %len(new_area)\n",
    "print 'Number of observations within the threshold is %d' %len(new_circ)\n",
    "\n",
    "# want to identify which position all the outliers are in so we can remove from all the lists \n",
    "# so could do a Boolean of T F whether in new_area or not and print the indices of the ones not? \n",
    "outliers_0 = []\n",
    "outliers_circ_0 = []\n",
    "print \"Outliers are gastruloids at the following indices, with the following areas: \"\n",
    "\n",
    "print \"Based on area: \"\n",
    "for (num,item) in enumerate(areas_0):\n",
    "    if item not in new_area:\n",
    "        print(num, item)\n",
    "        outliers_0.append(num)\n",
    "print \"Based on circularity: \"\n",
    "for (num,item) in enumerate(circularity_0):\n",
    "    if item not in new_circ:\n",
    "        print(num, item)\n",
    "        outliers_circ_0.append(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_indices = []\n",
    "for i, v in enumerate(filenames[0]):\n",
    "    file_indices.append(v)\n",
    "    \n",
    "file_indices = np.array(file_indices)\n",
    "file_indices = np.ndarray.tolist(file_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is only relevant in time courses \n",
    "outliers_24h = []\n",
    "#print \"outliers:\" # need to also add the outliers from the previous timepoint, if this is a time-course \n",
    "outliers = [10,12,27,55,56,88]\n",
    "prev_outliers = []\n",
    "for (num, item) in enumerate(areas_0):\n",
    "    if num in outliers_24h:\n",
    "        #print(num, item)\n",
    "        prev_outliers.append(num)\n",
    "\n",
    "#### Need change this depending on if the analysis is time-course or not \n",
    "\n",
    "# outliers_emptych = np.unique(outliers_0 + outliers_circ_0)\n",
    "toobig = []\n",
    "for i, v in enumerate(areas_0):\n",
    "    max = 1024 * 1024 \n",
    "    if v > 0.3 * max:\n",
    "        print i, v \n",
    "        toobig.append(i)\n",
    "\n",
    "\n",
    "outliers_emptych = np.unique(outliers_0 + outliers_circ_0 + toobig)\n",
    "print \"\\nCombined outliers are: \"\n",
    "print toobig\n",
    "print \"\\nBefore removing area outliers:\", len(areas_0)\n",
    "print \"Within based on area:\", len(new_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, sharex=False, figsize = (7, 7))\n",
    "\n",
    "ax[0,0].hist(areas_0, bins=50, density=None, \n",
    "         weights=None, cumulative=False, \n",
    "         histtype='bar', align='left', orientation='vertical', \n",
    "         rwidth=None, log=False, color='black', label= '# Observations', stacked=False, \n",
    "         normed=None, data=None)\n",
    "ax[0,1].hist(new_area, bins=50, density=None, \n",
    "             weights=None, cumulative=False, \n",
    "             histtype='bar', align='left', orientation='vertical', \n",
    "             rwidth=None, log=False, color='black', label= '# Observations', stacked=False, \n",
    "             normed=None, data=None)\n",
    "\n",
    "# add a 'best fit' line\n",
    "\n",
    "ax[0,0].set_xlabel('Area')\n",
    "ax[0,1].set_xlabel('Area')\n",
    "ax[0,0].set_ylabel('# Gastruloids')\n",
    "ax[0,0].set_title('Area Distribution of Gastruloids \\n with outliers')\n",
    "ax[0,1].set_title('Area Distribution of Gastruloids \\n no outliers')\n",
    "ax[1,0].set_title('Boxplot Area Distribution \\n with outliers')\n",
    "ax[1,1].set_title('Boxplot Area Distribution \\n no outliers')\n",
    "ax[1,0].set_ylabel('Area / # Pixels')\n",
    "ax[1,0].set_xlabel('Empty Channel')\n",
    "ax[1,1].set_xlabel('Empty Channel')\n",
    "\n",
    "new_area = np.array(new_area)\n",
    "clean_mean = np.mean(new_area, axis=0)\n",
    "ax[1,0].boxplot(areas_0, manage_xticks=True, autorange=False, zorder=None)\n",
    "ax[1,1].boxplot(new_area, manage_xticks=True, autorange=False, zorder=None)\n",
    "plt.tight_layout()\n",
    "\n",
    "data_locn = '/Users/alexandrabaranowski/Desktop/Images for report /'\n",
    "#plt.savefig(data_locn+'1 BraGFP 2016-03-16 72h Outlier removal1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting saturation artifacts - method: % of saturated pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbersaturated = []\n",
    "percentsaturated = []\n",
    "\n",
    "for k, v in enumerate(files_list[3][0]):  \n",
    "    # readthe original image\n",
    "    img = np.array(Image.open(files_list[3][0][k]))  \n",
    "\n",
    "    saturated = 255\n",
    "    white = np.sum(img == saturated)\n",
    "    numbersaturated.append(white)\n",
    "    width, height = img.shape\n",
    "    all = width*height\n",
    "    percent = 100.0*white/all\n",
    "    percentsaturated.append(percent)\n",
    "\n",
    "    #print \"Total pixels: %d\" % all, k\n",
    "   # print \"Saturated pixels: %d (%5.2f%%)\" % (white,percent)\n",
    "\n",
    "for i, v in enumerate(percentsaturated): \n",
    "    mean = np.mean(percentsaturated,axis = 0)\n",
    "    sd = np.std(percentsaturated, axis =0)\n",
    "    #print sd\n",
    "    \n",
    "    if v > mean + 2 * sd or v > 0.2: \n",
    "        print i, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 2. using the total intensity to find outliers #########\n",
    "sumofpixels1 = []\n",
    "channels = [0,1,2,3]\n",
    "newintensity = []\n",
    "#for j in range(0, len(channels)):\n",
    "for i, v in enumerate(blurred_masks_1):  \n",
    "    # readthe original image\n",
    "    #img = np.array(Image.open(files_list[3][0][i])) \n",
    "    img = v\n",
    "    x,y,z,a = cv2.sumElems(img)\n",
    "    sumofpixels1.append(x)\n",
    "\n",
    "\n",
    "mean = np.mean(sumofpixels1, axis = 0)\n",
    "sd = np.std(sumofpixels1, axis = 0)\n",
    "\n",
    "newintensity = [x for x in sumofpixels1 if (x > mean - 1.5 * sd)]\n",
    "newintensity = [x for x in newintensity if (x < mean + 1.5 * sd)]\n",
    "print newintensity\n",
    "\n",
    "outliers_intensity1 = []\n",
    "print \"Based on Sum of Pixels: \"\n",
    "for (num,item) in enumerate(sumofpixels1):\n",
    "    if item not in newintensity:\n",
    "        print(num, item)\n",
    "        outliers_intensity1.append(num)\n",
    "    \n",
    "        \n",
    "        #print sumofpixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ########### Using max and mean fluorescence to detect outliers ###########\n",
    "maxfluo3 \n",
    "meanfluo3\n",
    "\n",
    "### Max Fluorescence\n",
    "mean = np.mean(maxfluo1, axis = 0)\n",
    "sd = np.std(maxfluo1, axis = 0)\n",
    "\n",
    "newmaxfluo = [x for x in maxfluo1 if (x > mean - 1.5 * sd)]\n",
    "newmaxfluo = [x for x in newmaxfluo if (x < mean + 1.5 * sd)]\n",
    "#print newmaxfluo\n",
    "\n",
    "outliers_maxfluo1 = []\n",
    "print \"Based on Max Fluorescence: \"\n",
    "for (num,item) in enumerate(maxfluo1):\n",
    "    if item not in newmaxfluo:\n",
    "        print(num, item)\n",
    "        outliers_maxfluo1.append(num)\n",
    "print num, item\n",
    "\n",
    "### Mean Fluorescence\n",
    "mean2 = np.mean(meanfluo1, axis = 0)\n",
    "sd2 = np.std(meanfluo1, axis = 0)\n",
    "\n",
    "newmeanfluo = [x for x in meanfluo1 if (x > mean2 - 1.5 * sd2)]\n",
    "newmeanfluo = [x for x in newmeanfluo if (x < mean2 + 1.5 * sd2)]\n",
    "#print newmeanfluo\n",
    "\n",
    "outliers_meanfluo1 = []\n",
    "print \"Based on Mean Fluorescence: \"\n",
    "for (num,item) in enumerate(meanfluo1):\n",
    "    if item not in newmeanfluo:\n",
    "        print(num, item)\n",
    "        outliers_meanfluo1.append(num)\n",
    "print num, item\n",
    "\n",
    "### Standard Deviation of Fluorescence\n",
    "mean3 = np.mean(stdevfluo3, axis = 0)\n",
    "sd3 = np.std(stdevfluo3, axis = 0)\n",
    "\n",
    "newsdfluo = [x for x in stdevfluo3 if (x > mean3 - 2 * sd3)]\n",
    "newsdfluo = [x for x in newsdfluo if (x < mean3 + 2 * sd3)]\n",
    "#print newmeanfluo\n",
    "\n",
    "outliers_sdfluo1 = []\n",
    "print \"Based on St Dev of Fluorescence: \"\n",
    "for (num,item) in enumerate(stdevfluo1):\n",
    "    if item not in newsdfluo:\n",
    "        print(num, item)\n",
    "        outliers_sdfluo1.append(num)\n",
    "print num, item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_outliers = outliers_0 + outliers_circ_0\n",
    "\n",
    "##### When it is a time course and >24h AA, use this: \n",
    "total_outliers = outliers_0 + outliers_circ_0 \n",
    "\n",
    "total_outliers = np.unique(total_outliers)\n",
    "total_outliers = total_outliers[::-1]\n",
    "print total_outliers\n",
    "\n",
    "total_contour_areas = [areas_0, areas_1, areas_3]# areas_3]\n",
    "total_areas = [areas1, areas2, areas3]\n",
    "areas = [total_contour_areas, total_areas]\n",
    "\n",
    "#print total_areas\n",
    "\n",
    "for d in range(2):\n",
    "    for j in range(3):\n",
    "        for i in total_outliers: \n",
    "            area = areas[d][j]\n",
    "            del area[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(2): \n",
    "    parameter = areas[j]\n",
    "    for k in parameter:\n",
    "        #print len(k)\n",
    "        if len(k) != len(areas_0):\n",
    "            print \"error - check\"\n",
    "        else: \n",
    "            continue\n",
    "print \"check complete - all lengths correct \"\n",
    "print len(areas_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to now remove all those indices from all the other parameters too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimeters = [perimeters_0, perimeters_1, perimeters_3]\n",
    "circularity = [circularity_0, circularity_1, circularity_3]\n",
    "both = [perimeters, circularity]\n",
    "\n",
    "for k in range(2):\n",
    "    for j in range(3):\n",
    "        for i in total_outliers: \n",
    "            perimeter = both[k][j]\n",
    "            del perimeter[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing from the remaining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hulls = [hull_0, hull_1, hull_3]\n",
    "hull_areas = [hull_areas_0, hull_areas_1, hull_areas_3]\n",
    "convexity_defects = [convexity_defects_0, convexity_defects_1, convexity_defects_3]\n",
    "minc_radii = [minc_radii_0, minc_radii_1, minc_radii_3]\n",
    "boxes = [boxes_0, boxes_1, boxes_3]\n",
    "\n",
    "allparameters = [hulls, hull_areas, convexity_defects, minc_radii, boxes]\n",
    "for k in range(5):\n",
    "    for j in range(3):\n",
    "        for i in total_outliers: \n",
    "            parameter = allparameters[k][j]\n",
    "            del parameter[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These are arrays so can't delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments = [moments_0, moments_1, moments_3]\n",
    "hu = [hu_moments_0, hu_moments_1, hu_moments_3]\n",
    "loghu = [log_hu, log_hu1, log_hu3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More parameters to QC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroidy = [cy_0, cy_1, cy_3]\n",
    "#centroidx = [cx_0, cx_1, cx_3]\n",
    "ARs = [aspect_ratios_0, aspect_ratios_1, aspect_ratios_3]\n",
    "convexities = [convexity_0, convexity_1, convexity_3]\n",
    "\n",
    "\n",
    "moreparameters = [ ARs, convexities]#, majors, minors]\n",
    "\n",
    "for k in range(2): # how many in moreparameters\n",
    "    for j in range(3): # how many in each list\n",
    "        for i in total_outliers: \n",
    "            parameter1 = moreparameters[k][j]\n",
    "            del parameter1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_ellipses = [min_ellipse_0]#, min_ellipse_1, min_ellipse_3]\n",
    "majors = [major_ellipse_axis]# major_ellipse_axis1]#, major_ellipse_axis3]#, major_ellipse_axis3]\n",
    "minors = [minor_ellipse_axis]# minor_ellipse_axis1]#, minor_ellipse_axis3]\n",
    "\n",
    "ellipses_ones = [majors, minors]\n",
    "\n",
    "for k in range(2):\n",
    "    for j in range(1):\n",
    "        for i in total_outliers: \n",
    "            e = ellipses_ones[k][j]\n",
    "            del e[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you do not convert nan to num before, as you cannot delete from the array; so first delete and then remove nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_total = [I, I_1, I_3]\n",
    "logI_total = [log_i, log_ich1, log_ich3]\n",
    "\n",
    "individual_hu = [I_total, logI_total]\n",
    "\n",
    "for first in range(2):\n",
    "    for second in range(3): \n",
    "        for inside in range(7):\n",
    "            for i in total_outliers:\n",
    "                p = individual_hu[first][second][inside]\n",
    "                del p[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(aspect_ratios_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ich3 = np.nan_to_num(log_ich3)\n",
    "log_ich1 = np.nan_to_num(log_ich1)\n",
    "log_i13 = log_ich3[0]; log_i23 = log_ich3[1];log_i33 = log_ich3[2]; log_i43 = log_ich3[3]; log_i53 = log_ich3[4]; log_i63 = log_ich3[5]; log_i73 = log_ich3[6]\n",
    "log_i11 = log_ich1[0]; log_i21 = log_ich1[1];log_i31 = log_ich1[2]; log_i41 = log_ich1[3]; log_i51 = log_ich1[4]; log_i61 = log_ich1[5]; log_i71 = log_ich1[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Haralick features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_features = [H1ang2ndmoment, H2contrast, H3correlation, H4sumsqvar, H5invdiffmoment,H6sumavg,\n",
    "              H7sumvar, H8sumentropy, H9entropy, H10diffvar, H11diffentropy, H12imcorr1, H13imcorr2]\n",
    "\n",
    "hs = [H_features]\n",
    "\n",
    "for first in range(1):\n",
    "    for inside in range(13):\n",
    "        for i in total_outliers:\n",
    "            p = hs[first][inside]\n",
    "            del p[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### do a check for the individual hu as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_maxfluo1 = normalised_maxfluo1.tolist\n",
    "normalised_meanfluo1 = normalised_meanfluo1.tolist\n",
    "normalised_stdev1 = normalised_stdev1.tolist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix fluo intensity lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fluorescence = [maxfluo1, maxfluo2, maxfluo3]\n",
    "mean_fluorescence = [meanfluo1, meanfluo2, meanfluo3]\n",
    "st_dev_fluorescence = [stdevfluo1, stdevfluo2, stdevfluo3]\n",
    "\n",
    "#norm_max_fluorescence = [normalised_maxfluo1]#, maxfluo2, maxfluo3]\n",
    "#norm_mean_fluorescence = [normalised_meanfluo1]#, meanfluo2, meanfluo3]\n",
    "#norm_st_dev_fluorescence = [normalised_stdev1]#, stdevfluo2, stdevfluo3]\n",
    "\n",
    "intensities = [max_fluorescence, mean_fluorescence, st_dev_fluorescence]\n",
    "\n",
    "for k in range(3):\n",
    "    for j in range(3):\n",
    "        for i in total_outliers: \n",
    "            pixeli = intensities[k][j]\n",
    "            del pixeli[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_fluorescence = [minfluo1, minfluo2, minfluo3]\n",
    "\n",
    "for j in range(3):\n",
    "    for i in total_outliers:\n",
    "        mini = min_fluorescence[j]\n",
    "        del mini[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Channel 1 \n",
    "max_max1 = np.max(maxfluo1)\n",
    "normalised_maxfluo1 = (np.array(maxfluo1) / max_max1)\n",
    "#print normalised_maxfluo1\n",
    "\n",
    "max_mean1 = np.max(meanfluo1)\n",
    "normalised_meanfluo1 = (np.array(meanfluo1) / max_mean1)\n",
    "#print normalised_meanfluo1\n",
    "\n",
    "max_stdev1 = np.max(stdevfluo1) \n",
    "normalised_stdev1 = (np.array(stdevfluo1) / max_stdev1)\n",
    "#print normalised_stdev1\n",
    "\n",
    "### Channel 2 \n",
    "max_max2 = np.max(maxfluo2)\n",
    "normalised_maxfluo2 = (np.array(maxfluo2) / max_max2)\n",
    "#print normalised_maxfluo2\n",
    "\n",
    "max_mean2 = np.max(meanfluo2)\n",
    "normalised_meanfluo2 = (np.array(meanfluo2) / max_mean2)\n",
    "#print normalised_meanfluo2\n",
    "\n",
    "max_stdev2 = np.max(stdevfluo2) \n",
    "normalised_stdev2 = (np.array(stdevfluo2) / max_stdev2)\n",
    "#print normalised_stdev2\n",
    "\n",
    "### Channel 3 \n",
    "max_max3 = np.max(maxfluo3)\n",
    "normalised_maxfluo3 = (np.array(maxfluo3) / max_max3)\n",
    "#print normalised_maxfluo3\n",
    "\n",
    "max_mean3 = np.max(meanfluo3)\n",
    "normalised_meanfluo3 = (np.array(meanfluo3) / max_mean3)\n",
    "#print normalised_meanfluo3\n",
    "\n",
    "max_stdev3 = np.max(stdevfluo3) \n",
    "normalised_stdev3 = (np.array(stdevfluo3) / max_stdev3)\n",
    "#print normalised_stdev3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check all lengths are correct - this is for \"allparameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(5): \n",
    "    parameter = allparameters[j]\n",
    "    for k in parameter:\n",
    "        #print len(k)\n",
    "        if len(k) != len(areas_0):\n",
    "            print \"error - check\"\n",
    "        else: \n",
    "            continue\n",
    "print \"check complete - all lengths correct \"\n",
    "print len(circularity_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check all lengths are correct - this is for \"ellipses - ones\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(2): \n",
    "    e = ellipses_ones[j]\n",
    "    for k in e:\n",
    "        #print len(k)\n",
    "        if len(k) != len(areas_0):\n",
    "            print \"error - check\"\n",
    "        else: \n",
    "            continue\n",
    "print \"check complete - all lengths correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check all lengths are correct - this is for \"moreparameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(2): \n",
    "    parameter = moreparameters[j]\n",
    "    for k in parameter:\n",
    "        #print len(k)\n",
    "        if len(k) != len(areas_0):\n",
    "            print \"error - check\"\n",
    "        else: \n",
    "            continue\n",
    "print \"check complete - all lengths correct \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check all lengths are correct - this is for \"intensities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3): \n",
    "    pixeli = intensities[j]\n",
    "    for k in pixeli:\n",
    "        #print len(k)\n",
    "        if len(k) != len(areas_0):\n",
    "            print \"error - check\"\n",
    "        else: \n",
    "            continue\n",
    "print \"check complete - all lengths correct \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the areas of each channel and proportions of gastruloid covered by each marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the distribution (reproducibility) of the expression area & proportion of each marker across all the gastruloids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_0 = np.array(areas_0, dtype=np.float)\n",
    "areas1 = np.array(areas1, dtype=np.float)\n",
    "areas2 = np.array(areas2, dtype=np.float)\n",
    "areas3 = np.array(areas3, dtype=np.float)\n",
    "\n",
    "prop_ch1 = areas1 / areas_0\n",
    "prop_ch2 = areas2 / areas_0\n",
    "prop_ch3 = areas3 / areas_0\n",
    "\n",
    "print len(prop_ch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some preliminary graphs of the overlap \n",
    "\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "fig, axes = plt.subplots(3,3, figsize = (15,10), sharey=False)\n",
    "\n",
    "# this has drawn a histogram and plotted a kernel density estimate \n",
    "\n",
    "prop_ch1 = pd.Series(prop_ch1, name=\"Distribution of Sox2 area\")\n",
    "prop_ch2 = pd.Series(prop_ch2, name=\"Distribution of Sox17 area\")\n",
    "prop_ch3 = pd.Series(prop_ch3, name=\"Distribution of T/Bra area\")\n",
    "\n",
    "sns.distplot(prop_ch1, color = \"y\", ax=axes[0,0]) \n",
    "sns.distplot(prop_ch2, color = \"y\", ax=axes[0,1])\n",
    "sns.distplot(prop_ch3, color = \"y\", ax=axes[0,2])\n",
    "\n",
    "# this is a histogram with rugplot instead of kde;the rugplot adds a tick at each observation\n",
    "sns.distplot(prop_ch1, kde=False, rug=True, ax=axes[2,0])\n",
    "sns.distplot(prop_ch2, kde=False, rug=True, ax=axes[2,1])\n",
    "sns.distplot(prop_ch3, kde=False, rug=True, ax=axes[2,2])\n",
    "\n",
    "sns.kdeplot(prop_ch1, color = \"y\", ax=axes[1,0], shade=True)\n",
    "sns.kdeplot(prop_ch2, color = \"y\", ax=axes[1,1], shade=True)\n",
    "sns.kdeplot(prop_ch3, color = \"y\", ax=axes[1,2], shade=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "locn = data_locn_mask[0]\n",
    "plt.savefig(locn+\"Wnt3a final 20190429.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding overlap between the channels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Looking at just the areas that they overlap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ch1 and Ch2 \n",
    "percent12totalo = []\n",
    "ch121overlap = []\n",
    "ch122overlap = []\n",
    "percent12gld = []\n",
    "union12 = []\n",
    "\n",
    "for i, v in enumerate(areas_0):\n",
    "    ch1_area = areas_1[i]\n",
    "    ch2_area = areas2[i]\n",
    "    overlap12 =  np.float(cv2.bitwise_and(ch1_area,ch2_area)[0])\n",
    "    union_12area = ch1_area + ch2_area - overlap12\n",
    "    union12.append(union_12area)\n",
    "    \n",
    "    print '\\nUnion of areas', union_12area\n",
    "    print 'Ch1 area', ch1_area\n",
    "    print 'Ch2 area', ch2_area\n",
    "    print 'Overlap', overlap12\n",
    "    \n",
    "    # Overlap as percent of total area covered by those two \n",
    "    try:\n",
    "        percent12total = overlap12 / union_12area\n",
    "        ch121covered = overlap12 / ch1_area \n",
    "        ch122covered = overlap12 / ch2_area\n",
    "        percent12gldd = overlap12 / v\n",
    "        \n",
    "        percent12totalo.append(percent12total*100)\n",
    "        ch121overlap.append(ch121covered*100)\n",
    "        ch122overlap.append(ch122covered*100)\n",
    "        percent12gld.append(percent12gldd*100)\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        percent12total = 0\n",
    "        ch121covered = 0 \n",
    "        ch122covered = 0 \n",
    "        percent12gldd = 0\n",
    "        \n",
    "        percent12totalo.append(0)\n",
    "        ch121overlap.append(0)\n",
    "        ch122overlap.append(0)\n",
    "        percent12gld.append(0)\n",
    "    \n",
    "    print '% Union Area 1 and 2 covered', percent12total *100\n",
    "    \n",
    "    # How much of Ch1 area it covers \n",
    "    print '% Ch1 covered by overlap', ch121covered*100\n",
    "    \n",
    "    # How much of Ch2 area it covers \n",
    "    print '% Ch2 covered by overlap', ch122covered*100\n",
    "    # Overlap as percent of gastruloid area\n",
    "    print '% Gldcovered by overlap', percent12gldd*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ch2 and Ch3 \n",
    "percent23totalo = []\n",
    "ch232overlap = []\n",
    "ch233overlap = []\n",
    "percent23gld = []\n",
    "union23 = []\n",
    "\n",
    "for i, v in enumerate(areas_0):\n",
    "    ch2_area = areas2[i]\n",
    "    ch3_area = areas_3[i]\n",
    "    overlap23 =  np.float(cv2.bitwise_and(ch2_area,ch3_area)[0])\n",
    "    union_23area = ch2_area + ch3_area - overlap23\n",
    "    union23.append(union_23area)\n",
    "    \n",
    "    print '\\nUnion of areas', union_23area\n",
    "    print 'Ch2 area', ch2_area\n",
    "    print 'Ch3 area', ch3_area\n",
    "    print 'Overlap', overlap23\n",
    "  \n",
    "    try:\n",
    "        percent23total = overlap23 / union_23area\n",
    "        ch232covered = overlap23 / ch2_area\n",
    "        ch233covered = overlap23 / ch3_area\n",
    "        percent23gldd = overlap23 / v\n",
    "        \n",
    "        percent23totalo.append(percent23total*100)\n",
    "        ch232overlap.append(ch232covered*100)\n",
    "        ch233overlap.append(ch233covered*100)\n",
    "        percent23gld.append(percent23gldd*100)\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        percent23total = 0\n",
    "        ch232covered = 0 \n",
    "        ch233covered = 0 \n",
    "        percent23gldd = 0\n",
    "        \n",
    "        percent23totalo.append(0)\n",
    "        ch232overlap.append(0)\n",
    "        ch233overlap.append(0)\n",
    "        percent23gld.append(0)\n",
    "    \n",
    "    print '% of Union of Areas covered', percent23total*100\n",
    "    \n",
    "    # How much of Ch1 area it covers \n",
    "    print '% Ch2 covered by overlap', ch232covered*100\n",
    "    \n",
    "    # How much of Ch2 area it covers \n",
    "    print '% Ch3 covered by overlap', ch233covered *100\n",
    "    \n",
    "    # Overlap as percent of gastruloid area\n",
    "    print '% Gld covered by overlap', percent23gldd *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ch1 and Ch3 \n",
    "percent13totalo = []\n",
    "ch131overlap = []\n",
    "ch133overlap = []\n",
    "percent13gld = []\n",
    "union13 = []\n",
    "\n",
    "for i, v in enumerate(areas_0):\n",
    "    ch1_area = areas_1[i]\n",
    "    ch3_area = areas_3[i]\n",
    "    overlap13 =  np.float(cv2.bitwise_and(ch1_area,ch3_area)[0])\n",
    "    union_13area = ch1_area + ch3_area - overlap13\n",
    "    union13.append(union_13area)\n",
    "    \n",
    "    print '\\nUnion of areas', union_13area\n",
    "    print 'Ch1 area', ch1_area\n",
    "    print 'Ch3 area', ch3_area\n",
    "    print 'Overlap', overlap13\n",
    "\n",
    "    try:\n",
    "        percent13total = overlap13/ union_13area\n",
    "        ch131covered = overlap13 / ch1_area\n",
    "        ch133covered = overlap13 / ch3_area\n",
    "        percent13gldd = overlap13 / v\n",
    "        \n",
    "        percent13totalo.append(percent13total)\n",
    "        ch131overlap.append(ch131covered)\n",
    "        ch133overlap.append(ch133covered)\n",
    "        percent13gld.append(percent13gldd)\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        percent13total = 0\n",
    "        ch131covered = 0 \n",
    "        ch133covered = 0 \n",
    "        percent13gldd = 0\n",
    "        \n",
    "        percent13totalo.append(0)\n",
    "        ch131overlap.append(0)\n",
    "        ch133overlap.append(0)\n",
    "        percent13gld.append(0)\n",
    "    \n",
    "    print '% of Union of Areas covered', percent13total*100\n",
    "    \n",
    "    # How much of Ch1 area it covers \n",
    "    print '% Ch1 covered by overlap', ch131covered*101\n",
    "    \n",
    "    # How much of Ch2 area it covers \n",
    "    print '% Ch3 covered by overlap', ch133covered *100\n",
    "    \n",
    "    # Overlap as percent of gastruloid area\n",
    "    print '% Gld covered by overlap', percent13gldd *100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### plot pixel intensity values against each other \n",
    "max1 = []\n",
    "ones = []\n",
    "max2 = []\n",
    "twos = []\n",
    "for i, v in enumerate(files_list[1][0][0:2]):\n",
    "    img1 = Image.open(files_list[1][0][0:2][i])\n",
    "    ones.append(img1)\n",
    "    img2 = Image.open(files_list[2][0][0:2][i])\n",
    "    twos.append(img2)\n",
    "    \n",
    "    max1.append(np.max(img1))\n",
    "    max2.append(np.max(img2))\n",
    "\n",
    "maxmax1 = np.max(max1)\n",
    "maxmax2 = np.max(max2)\n",
    "\n",
    "for i, v in enumerate(ones):\n",
    "    img1 = np.array(v) / maxmax1\n",
    "    \n",
    "    img2 = np.array(twos[i]) / maxmax2\n",
    "    \n",
    "    \n",
    "    plt.scatter(img1, img2, alpha = 0.5)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(blurred_2[0:1]):\n",
    "    #img = np.array(Image.open(files_list[1][0][0:2][i]))\n",
    "    img = np.array(v)\n",
    "    img1 = img.copy()\n",
    "\n",
    "    imgg = np.array(blurred_3[i])\n",
    "    img2 = imgg.copy()\n",
    "    #img2 = cv2.GaussianBlur(img2, (15,15), 0)\n",
    "    \n",
    "    data1 = np.array(img1)\n",
    "    data2 = np.array(img2)\n",
    "    \n",
    "    #colors = ('data1', 'data2')\n",
    "    #fig, ax = plt.subplot(1,1, figsize = (10,3))\n",
    "    \n",
    "    plt.scatter(data1, data2, alpha = 0.1, cmap='viridis')\n",
    "    plt.xlim((0,255))\n",
    "    plt.ylim((0,255))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = blurred_2[0]\n",
    "y = blurred_3[0]\n",
    "\n",
    "def r2(x1, y1):\n",
    "    return np.float(stats.pearsonr(x, y)[0] ** 2)\n",
    "sns.jointplot(x, y, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a dataframe to add all the parameters we have, with each parameter as a column and each gld as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'TreatmentConditions': 'Drug Experiments','Date': '2019-03-04', 'Hrs Post A': '72', \n",
    "      'Cell line': 'RUES2', 'Pre treatment': 'Wnt3a', 'Treatment': '0.5 Chi 3 Chi', 'Cell number': '400',\n",
    "     'Area Ch0': areas_0, 'Total Area Ch1': areas1, 'Contour Area Ch1': areas_1, \n",
    "     'Total Area Ch2': areas2, 'Total Area Ch3': areas3, 'Contour Area Ch3': areas_3,\n",
    "      'Perimeter Ch0': perimeters_0,'Perimeter Ch1': perimeters_1, 'Perimeter Ch3': perimeters_3, \n",
    "    'Circularity Ch0': circularity_0, 'Circularity Ch1': circularity_1,'Circularity Ch3': circularity_3,\n",
    "     'Min rot rect Ch0': boxes_0, 'Min rot rect Ch1': boxes_1, 'Min rot rect Ch3': boxes_3, \n",
    "     'Proportion Ch1': prop_ch1, 'Proportion Ch2': prop_ch2, 'Proportion Ch3': prop_ch3, \n",
    "    'MECradius Ch0': minc_radii_0, 'MECradius Ch1': minc_radii_1, 'MECradius Ch3': minc_radii_3,\n",
    "     'Hu I1 Ch0': I1, 'Hu I2 Ch0': I2,'Hu I3 Ch0': I3,'Hu I4 Ch0': I4,'Hu I5 Ch0': I5,'Hu I6 Ch0': I6,'Hu I7 Ch0': I7,\n",
    "     'Hu I1 Ch1': I1_1, 'Hu I2 Ch1': I2_1,'Hu I3 Ch1': I3_1,'Hu I4 Ch1': I4_1,'Hu I5 Ch1': I5_1,'Hu I6 Ch1': I6_1,'Hu I7 Ch1': I7_1,\n",
    "     'Hu I1 Ch3': I1_3, 'Hu I2 Ch3': I2_3,'Hu I3 Ch3': I3_3,'Hu I4 Ch3': I4_3,'Hu I5 Ch3': I5_3,'Hu I6 Ch3': I6_3,'Hu I7 Ch3': I7_3,\n",
    "     'Log Hu I1 Ch0': log_i1, 'Log Hu I2 Ch0': log_i2,'Log Hu I3 Ch0': log_i3,'Log Hu I4 Ch0': log_i4,'Log Hu I5 Ch0': log_i5,'Log Hu I6 Ch0': log_i6,'Log Hu I7 Ch0': log_i7,\n",
    "     'Log Hu I1 Ch1': log_i11, 'Log Hu I2 Ch1': log_i21,'Log Hu I3 Ch1': log_i31,'Log Hu I4 Ch1': log_i41,'Log Hu I5 Ch1': log_i51,'Log Hu I6 Ch1': log_i61,'Log Hu I7 Ch1': log_i71,\n",
    "     'Log Hu I1 Ch3': log_i13, 'Log Hu I2 Ch3': log_i23,'Log Hu I3 Ch3': log_i33,'Log Hu I4 Ch1': log_i43,'Log Hu I5 Ch3': log_i53,'Log Hu I6 Ch3': log_i63,'Log Hu I7 Ch3': log_i73,\n",
    "     'Hull Area Ch0': hull_areas_0, 'Hull Area Ch1': hull_areas_1,'Hull Area Ch3': hull_areas_3,\n",
    "     'Convexity Ch0': convexity_0, 'Convexity Ch1': convexity_1,'Convexity Ch3': convexity_3,\n",
    "     'AR Ch0': aspect_ratios_0, 'AR Ch1': aspect_ratios_1,'AR Ch3': aspect_ratios_3,\n",
    "      'H1Ang2ndmoment': H1ang2ndmoment, 'H2Contrast': H2contrast, \n",
    "     'H3 Correlation': H3correlation, 'H4 Sum Sq Var': H4sumsqvar, 'H5 Ind Diff Moment': H5invdiffmoment,\n",
    "     'H6 Sum Avg': H6sumavg,'H7 Sum Var': H7sumvar, 'H8 Sum Entropy': H8sumentropy, 'H9 Entropy': H9entropy, \n",
    "     'H10 Diff Var': H10diffvar, 'H11 Diff Entropy': H11diffentropy, 'H12 Im Corr 1': H12imcorr1,  \n",
    "     'H13 Im Corr 2': H13imcorr2,'Norm Max Fluorescence Ch1': normalised_maxfluo1, 'Norm Mean Fluorescence Ch1': normalised_meanfluo1,\n",
    "     'Norm St Dev Fluorescence Ch1': normalised_stdev1, 'Norm Max Fluorescence Ch2': normalised_maxfluo2, 'Norm Mean Fluorescence Ch2': normalised_meanfluo2,\n",
    "     'Norm St Dev Fluorescence Ch2': normalised_stdev2, 'Norm Max Fluorescence Ch3': normalised_maxfluo3, 'Norm Mean Fluorescence Ch3': normalised_meanfluo3,\n",
    "     'Norm St Dev Fluorescence Ch3': normalised_stdev3, 'Overlap of 1 3 %1 3area':percent13totalo,'Ch1 3 Overlap %Ch1 Fluo':ch131overlap,\n",
    "     'Ch1 3 Overlap % Ch3 Fluo':ch133overlap, 'Ch 1 3 Overlap % Gld': percent13gld,'Overlap of 1 2 %1 2area':percent12totalo,'Ch1 2 Overlap %Ch1 Fluo':ch121overlap,\n",
    "     'Ch1 2 Overlap % Ch2 Fluo':ch122overlap, 'Ch 1 2 Overlap % Gld': percent12gld,'Overlap of 2 3 %2 3area':percent23totalo,'Ch2 3 Overlap %Ch2 Fluo':ch232overlap,\n",
    "     'Ch2 3 Overlap % Ch3 Fluo':ch233overlap, 'Ch 2 3 Overlap % Gld': percent23gld, 'St Dev Fluorescence Ch1': stdevfluo1, 'Max Fluorescence Ch2': maxfluo2, 'Mean Fluorescence Ch2': meanfluo2,\n",
    "     'St Dev Fluorescence Ch2': stdevfluo2, 'Max Fluorescence Ch3': maxfluo3, 'Mean Fluorescence Ch3': meanfluo3, 'St Dev Fluorescence Ch3': stdevfluo3, 'Max Fluorescence Ch1': maxfluo1,\n",
    "     'Mean Fluorescence Ch1': meanfluo1, 'Union Ch1 Ch2': union12, 'Union Ch2 Ch3': union23, 'Union Ch1 Ch3': union13\n",
    "    }\n",
    "df = pd.DataFrame(data=d)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/alexandrabaranowski/Desktop/Project Code/20192904/'\n",
    "\n",
    "df.to_csv(path+'2019-03-04 Wnt3a 72h 2904.csv') ## this saves it to the Project Code Folder - find a way to \n",
    "# save it to a more clear folder and for it to automatically take the name of the original folder\n",
    "\n",
    "################ N.B. Because conditions are added manually to the df the subgroups are not added at this point, \n",
    "#indices that were removed need to then be accounted for when the individual subgroups are added to the csv file manually;\n",
    "# indices can be found in the first cell of the 'Removing Outliers' section ###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
